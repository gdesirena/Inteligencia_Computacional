{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd98cff3",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 3: Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eada52",
   "metadata": {},
   "source": [
    "En aprendizaje por refuerzo (ó Reinforcement Learning en inglés) no tenemos una “etiqueta de salida”, por lo que no es de tipo supervisado y si bien estos algoritmos aprenden por sí mismos, tampoco son de tipo no supervisado, en donde se intenta clasificar grupos teniendo en cuenta alguna distancia entre muestras.\n",
    "\n",
    "En el mundo real contamos con múltiples variables que por lo general se interrelacionan y que dependen de otras variables y dan lugar a escenarios más grandes en donde tomar decisiones. Para conducir un coche no basta una inteligencia que pueda detectar un semáforo en rojo, verde ó amarillo; tendremos muchísimos factores -todos a la vez- a los que prestar atención: a qué velocidad vamos, estamos ante una curva?, hay peatones?, es de noche y debemos encender las luces?.\n",
    "\n",
    "Una solución sería tener múltiples máquinas de ML supervisadas y que interactúan entre si -y esto no estaría mal- ó podemos cambiar el enfoque… Y ahí aparece el Reinforcement Learning (RL) como una alternativa, tal vez de las más ambiciosas en las que se intenta integrar el Machine Learning en el mundo real, sobre todo aplicado a robots y maquinaria industrial.\n",
    "\n",
    "El **Reinforcement Learning entonces**, intentará hacer aprender a la máquina basándose en un esquema de “premios y castigos” -cómo con el perro de Pablov- en un entorno en donde hay que tomar acciones y que está afectado por múltiples variables que cambian con el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e94bc",
   "metadata": {},
   "source": [
    "## Componentes del RL\n",
    "\n",
    "El **Reinforcement Learning** propone un nuevo enfoque para hacer que nuestra máquina aprenda, para ello, postula los siguientes 2 componentes:\n",
    "\n",
    "> el `Agente`: será nuestro modelo que queremos entrenar y que aprenda a tomar decisiones.\n",
    "\n",
    "> `Ambiente`: será el entorno en donde interactúa y “se mueve” el agente. El ambiente contiene las limitaciones y reglas posibles a cada momento.\n",
    "\n",
    "Entre ellos hay una relación que se retroalimenta y cuenta con los siguientes nexos:\n",
    "\n",
    "> `Acción`: las posibles acciones que puede tomar en un momento determinado el Agente.\n",
    "\n",
    "> `Estado (del ambiente)`: son los indicadores del ambiente de cómo están los diversos elementos que lo componen en ese momento.\n",
    "\n",
    "> `Recompensas (ó castigos!)`: a raíz de cada acción tomada por el Agente, podremos obtener un premio ó una penalización que orientarán al Agente en si lo está haciendo bien ó mal.\n",
    "\n",
    "![title](Figures/mdp_diagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302f887",
   "metadata": {},
   "source": [
    "En un primer momento, el agente recibe un estado inicial y toma una acción con lo cual influye é interviene en el ambiente. Esto está muy bien, pues es muy cierto que cuando tomamos decisiones en el mundo real lo estamos modificando, ¿no?. Y esa decisión tendrá sus consecuencias: en la siguiente iteración el ambiente devolverá al agente el nuevo estado y la recompensa obtenida. Si la recompensa es positiva estaremos reforzando ese comportamiento para el futuro. En cambio si la recompensa es negativa lo estaremos penalizando, para que ante la misma situación el agente actúe de manera distinta. El esquema en el que se apoya el Reinforcement Learning es en el de **Proceso de Decisión de Markov**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec91e6d",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Ahora vamos a comentar uno de los modelos usados en Reinforcement Learning para poder concretar un ejemplo de su implementación. Es el llamado “Q-Learning”.\n",
    "\n",
    "Repasemos los elementos que tenemos:\n",
    "\n",
    "> Políticas: Es una tabla (aunque puede tener n-dimensiones) que le indicará al modelo “como actuar” en cada estado.\n",
    "\n",
    "> Acciones: las diversas elecciones que puede hacer el agente en cada estado\n",
    "\n",
    "> Recompensas: si sumamos ó restamos puntaje con la acción tomada\n",
    "Comportamiento “avaro” (greedy en inglés) del agente. Es decir, si se dejará llevar por grandes recompensas inmediatas, ó irá explorando y valorando las riquezas a largo plazo\n",
    "\n",
    "El objetivo principal al entrenar nuestro modelo a través de las simulaciones será ir “rellenando” la tabla de Políticas de manera que las decisiones que vaya tomando nuestro agente obtengan “la mayor recompensa” a la vez que avanzamos y no nos quedamos estancados, es decir, pudiendo cumplir el objetivo global (ó final) que deseamos alcanzar.\n",
    "\n",
    "A la política la llamaremos “Q” por lo que:\n",
    "\n",
    "```\n",
    "Q(estado, acción) nos indicará el valor de la política para un estado y una acción determinados.\n",
    "```\n",
    "\n",
    "Y para saber cómo ir completando la tabla de políticas nos valemos de la ecuación de Bellman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1a5a3",
   "metadata": {},
   "source": [
    "$$ \\hat{Q}(s,a) = Q(s,a) + \\alpha [R +(\\lambda \\max Q(s')) - Q(s,a)]$$\n",
    "\n",
    "- $Q(s,a)$ valor actual\n",
    "- $\\alpha$ ratio de aprendizaje\n",
    "- $R$ Recompensa\n",
    "- $\\lambda$ tasa descuento\n",
    "- $\\max Q(s')$ valor óptimo esperado\n",
    "- $Q(s,a)$ valor actual\n",
    "\n",
    "La idea es ir actualizando las políticas Q^(s,a) en base al valor actual más una futura recompensa que recibiremos, en caso de tomar dicha acción. Hay dos ratios que afectan a la manera en que influye esa recompensa: el ratio de aprendizaje, que regula “la velocidad” en la que se aprende, y la “tasa de descuento” que tendrá en cuenta la recompensa a corto o largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4744f",
   "metadata": {},
   "source": [
    "## La librería Gym:\n",
    "\n",
    "Gym es una librería para aplicaciones de aprendizaje reforzado. Nos proporciona una interfaz sencilla para un gran número de tareas, entre ellas\n",
    "\n",
    "- Tareas de control clásicas (CartPole, Pendulum, MountainCar, etc.)\n",
    "- Videojuegos clásicos (Space Invaders, Breakout, Pong, etc.)\n",
    "- Tareas de control continuo\n",
    "- Manipulación de brazo robótico\n",
    "\n",
    "En esta sección vamos a familiarizarnos con los cinco métodos que usaremos al resolver un problema de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ccf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b055a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from envs import Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c9966",
   "metadata": {},
   "source": [
    "\n",
    "##### Creando el entorno: Maze()\n",
    "\n",
    "Para crear un entorno, simplemente pase una cadena con su nombre al método gym.make. Si el entorno existe, el método devuelve una instancia de la clase gym.Env, que representa el entorno de la tarea que vamos a resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d584ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b35c1a",
   "metadata": {},
   "source": [
    "###### env.reset()\n",
    "\n",
    "Este método pone el entorno en su estado inicial y lo devuelve para que el agente pueda observarlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b056e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new episode will start in state: (0, 0)\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(f\"The new episode will start in state: {initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb9991",
   "metadata": {},
   "source": [
    "##### env.render()\n",
    "\n",
    "Este método genera una imagen que representa el estado actual del entorno, en forma de np.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4171822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.classic_control import rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee79f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee798f7160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {initial_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6d7f3",
   "metadata": {},
   "source": [
    "##### env.step()\n",
    "\n",
    "Este método aplica la acción seleccionada por el agente en el entorno, para modificarlo. En respuesta, el entorno devuelve una tupla de cuatro objetos:\n",
    "\n",
    "- El siguiente estado\n",
    "- La recompensa obtenida\n",
    "- (bool) si la tarea ha sido completada\n",
    "- cualquier otra información relevante en un diccionario de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5a62a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After moving down 1 row, the agent is in state: (1, 0)\n",
      "After moving down 1 row, we got a reward of: -1.0\n",
      "After moving down 1 row, the task is not finished\n"
     ]
    }
   ],
   "source": [
    "action = 2\n",
    "next_state, reward, done, info = env.step(action)\n",
    "print(f\"After moving down 1 row, the agent is in state: {next_state}\")\n",
    "print(f\"After moving down 1 row, we got a reward of: {reward}\")\n",
    "print(\"After moving down 1 row, the task is\", \"\" if done else \"not\", \"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3416da5",
   "metadata": {},
   "source": [
    "###### Renderizar el nuevo estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef58c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ee7bc6a160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = env.render(mode='rgb_array')\n",
    "plt.axis('off')\n",
    "plt.title(f\"State: {next_state}\")\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c64c52",
   "metadata": {},
   "source": [
    "##### env.close()\n",
    "\n",
    "Completa la tarea y cierra el entorno, liberando los recursos asociados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884bab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da000168",
   "metadata": {},
   "source": [
    "## Maze environment: Find the exit.\n",
    "\n",
    "\n",
    "In this section we are going to familiarize ourselves with the environment that we'll use in the lessons 2 (dynamic programming), 3 (Monte Carlo methods) and 4 (temporal difference methods). This environment is perfect for learning the basics of Reinforcement Learning because:\n",
    "\n",
    "- It has few states (25)\n",
    "- Transitions between states are deterministic ($p(s', r| s, a) = 1$)\n",
    "- All rewards are the same (-1) until the episode concludes. Thus facilitating the study of the value and action-value functions\n",
    "\n",
    "Through this environment, we are going to review the concepts seen in lesson 1 (The Markov decision process):\n",
    "\n",
    "- States and state space\n",
    "- Actions and action space\n",
    "- Trajectories and episodes\n",
    "- Rewards and returns\n",
    "- Policy\n",
    "\n",
    "\n",
    "The environment is a maze of 5x5 cells, in which the goal of the agent is to find the exit, located in the lower right corner, in the cell (4,4). In the image, the exit is colored in light green.\n",
    "\n",
    "To reach the exit, the agent can take four different actions: move up, move down, move left and move right.\n",
    "\n",
    "\n",
    "En esta sección vamos a familiarizarnos con el entorno para el algoritmo de programación dinámica, métodos Monte Carlo y métodos de diferencia temporal. Este entorno es perfecto para aprender los conceptos básicos del aprendizaje por refuerzo porque:\n",
    "\n",
    "- Tiene pocos estados (25)\n",
    "- Las transiciones entre estados son deterministas ($p(s', r| s, a) = 1$)\n",
    "- Todas las recompensas son iguales (-1) hasta que concluye el episodio. Facilitando así el estudio de las funciones valor y acción-valor\n",
    "\n",
    "A través de este entorno, vamos a repasar los conceptos del proceso de decisión de Markov:\n",
    "\n",
    "- Estados y espacio de estados\n",
    "- Acciones y espacio de acción.\n",
    "- Trayectorias y episodios\n",
    "- Recompensas y devoluciones\n",
    "- Política\n",
    "\n",
    "\n",
    "El entorno es un laberinto de celdas de 5x5, en el que el objetivo del agente es encontrar la salida, ubicada en la esquina inferior derecha, en la celda (4,4). En la imagen, la salida está coloreada en verde claro.\n",
    "\n",
    "Para llegar a la salida, el agente puede realizar cuatro acciones diferentes: moverse hacia arriba, moverse hacia abajo, moverse hacia la izquierda y moverse hacia la derecha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd30ce",
   "metadata": {},
   "source": [
    "###### Creando el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df316aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1426b",
   "metadata": {},
   "source": [
    "##### Estados y espacio de estados\n",
    "\n",
    "Los estados consisten en una tupla de dos números enteros, ambos en el rango [0, 4], que representan la fila y la columna en la que se encuentra actualmente el agente:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "    s = (row, column) \\;\\\\\n",
    "    row, column \\in \\{0,1,2,3, 4\\}\n",
    "\\end{equation}\n",
    "<br>\n",
    "<br>\n",
    "El espacio de estado (conjunto de todos los estados posibles en la tarea) tiene 25 elementos (todas las combinaciones posibles de filas y columnas):\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    Rows \\times Columns \\;\\\\\n",
    "    S = \\{(0, 0), (0, 1), (1, 0), ...\\}\n",
    "\\end{equation}\n",
    "\n",
    "La información sobre el espacio de estado se almacena en la propiedad env.observation_space. En este entorno, es de tipo MultiDiscrete([5 5]), lo que significa que consta de dos elementos (filas y columnas), cada uno con 5 valores diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a013ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, the initial state is: (0, 0)\n",
      "The space state is of type: MultiDiscrete([5 5])\n"
     ]
    }
   ],
   "source": [
    "print(f\"For example, the initial state is: {env.reset()}\")\n",
    "print(f\"The space state is of type: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b0e62",
   "metadata": {},
   "source": [
    "##### Acciones y espacio de acción\n",
    "\n",
    "En este entorno, hay cuatro acciones diferentes y se representan con números enteros:\n",
    "\n",
    "\\begin{equation}\n",
    "a \\in \\{0, 1, 2, 3\\}\n",
    "\\end{equation}\n",
    "\n",
    "- 0 -> move up\n",
    "- 1 -> move right\n",
    "- 2 -> move down\n",
    "- 3 -> move left\n",
    "\n",
    "Para ejecutar una acción, simplemente pásela como argumento al método env.step. La información sobre el espacio de acción se almacena en la propiedad env.action_space que es de clase Discreto (4). Esto significa que en este caso solo consta de un elemento en el rango [0,4), a diferencia del espacio de estado que se ve arriba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "963f498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a valid action is: 1\n",
      "The action state is of type: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"An example of a valid action is: {env.action_space.sample()}\")\n",
    "print(f\"The action state is of type: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f36490",
   "metadata": {},
   "source": [
    "##### Trayectorias y episodios\n",
    "\n",
    "Una trayectoria es la secuencia generada al pasar de un estado a otro (ambos arbitrarios)\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\n",
    "\\end{equation}\n",
    "\n",
    "Generemos una trayectoria de 3 movimientos en código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9397e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first trajectory:\n",
      "[[(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)]]\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "trajectory = []\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    trajectory.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"first trajectory:\\n{trajectory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bbdb47",
   "metadata": {},
   "source": [
    "Un episodio es una trayectoria que va desde el estado inicial del proceso hasta el final:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\n",
    "\\end{equation}\n",
    "donde T es el estado terminal.\n",
    "\n",
    "Generemos un episodio completo en código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "623606f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first episode:\n",
      "[[(0, 0), 0, -1.0, False, (0, 0)], [(0, 0), 0, -1.0, False, (0, 0)], [(0, 0), 0, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 2, -1.0, False, (1, 1)], [(1, 1), 1, -1.0, False, (1, 1)], [(1, 1), 0, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 2, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 0, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 0, -1.0, False, (0, 0)], [(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 2, -1.0, False, (0, 2)], [(0, 2), 0, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 1, -1.0, False, (0, 4)], [(0, 4), 3, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 2, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 0, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 1, -1.0, False, (0, 4)], [(0, 4), 3, -1.0, False, (0, 3)], [(0, 3), 1, -1.0, False, (0, 4)], [(0, 4), 2, -1.0, False, (1, 4)], [(1, 4), 3, -1.0, False, (1, 4)], [(1, 4), 3, -1.0, False, (1, 4)], [(1, 4), 3, -1.0, False, (1, 4)], [(1, 4), 1, -1.0, False, (1, 4)], [(1, 4), 2, -1.0, False, (2, 4)], [(2, 4), 2, -1.0, False, (2, 4)], [(2, 4), 2, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 3, -1.0, False, (2, 4)], [(2, 4), 2, -1.0, False, (2, 4)], [(2, 4), 0, -1.0, False, (1, 4)], [(1, 4), 2, -1.0, False, (2, 4)], [(2, 4), 0, -1.0, False, (1, 4)], [(1, 4), 1, -1.0, False, (1, 4)], [(1, 4), 2, -1.0, False, (2, 4)], [(2, 4), 2, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 3, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 2, -1.0, False, (2, 4)], [(2, 4), 3, -1.0, False, (2, 4)], [(2, 4), 3, -1.0, False, (2, 4)], [(2, 4), 1, -1.0, False, (2, 4)], [(2, 4), 0, -1.0, False, (1, 4)], [(1, 4), 0, -1.0, False, (0, 4)], [(0, 4), 3, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 0, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 1, -1.0, False, (0, 4)], [(0, 4), 1, -1.0, False, (0, 4)], [(0, 4), 1, -1.0, False, (0, 4)], [(0, 4), 0, -1.0, False, (0, 4)], [(0, 4), 0, -1.0, False, (0, 4)], [(0, 4), 0, -1.0, False, (0, 4)], [(0, 4), 3, -1.0, False, (0, 3)], [(0, 3), 1, -1.0, False, (0, 4)], [(0, 4), 1, -1.0, False, (0, 4)], [(0, 4), 0, -1.0, False, (0, 4)], [(0, 4), 2, -1.0, False, (1, 4)], [(1, 4), 1, -1.0, False, (1, 4)], [(1, 4), 0, -1.0, False, (0, 4)], [(0, 4), 3, -1.0, False, (0, 3)], [(0, 3), 2, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 1, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 1, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 1, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 1, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 3, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 0, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 2, -1.0, False, (1, 1)], [(1, 1), 2, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 0, -1.0, False, (1, 1)], [(1, 1), 3, -1.0, False, (1, 1)], [(1, 1), 3, -1.0, False, (1, 1)], [(1, 1), 1, -1.0, False, (1, 1)], [(1, 1), 1, -1.0, False, (1, 1)], [(1, 1), 2, -1.0, False, (2, 1)], [(2, 1), 3, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 2, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 3, -1.0, False, (2, 1)], [(2, 1), 0, -1.0, False, (1, 1)], [(1, 1), 1, -1.0, False, (1, 1)], [(1, 1), 3, -1.0, False, (1, 1)], [(1, 1), 2, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 3, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 3, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 2, -1.0, False, (3, 1)], [(3, 1), 1, -1.0, False, (3, 1)], [(3, 1), 0, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 1, -1.0, False, (2, 1)], [(2, 1), 0, -1.0, False, (1, 1)], [(1, 1), 0, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 2, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 0, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 2, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 1, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 2, -1.0, False, (2, 3)], [(2, 3), 2, -1.0, False, (2, 3)], [(2, 3), 3, -1.0, False, (2, 2)], [(2, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 0, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 1, -1.0, False, (1, 2)], [(1, 2), 3, -1.0, False, (1, 2)], [(1, 2), 2, -1.0, False, (2, 2)], [(2, 2), 2, -1.0, False, (2, 2)], [(2, 2), 1, -1.0, False, (2, 3)], [(2, 3), 0, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 3, -1.0, False, (1, 3)], [(1, 3), 0, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 0, -1.0, False, (0, 2)], [(0, 2), 1, -1.0, False, (0, 3)], [(0, 3), 0, -1.0, False, (0, 3)], [(0, 3), 3, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 1, -1.0, False, (0, 2)], [(0, 2), 3, -1.0, False, (0, 1)], [(0, 1), 2, -1.0, False, (1, 1)], [(1, 1), 0, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)], [(0, 0), 0, -1.0, False, (0, 0)], [(0, 0), 2, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 0, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 3, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 3, -1.0, False, (2, 0)], [(2, 0), 0, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 0, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 3, -1.0, False, (2, 0)], [(2, 0), 0, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 0, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 0, -1.0, False, (1, 0)], [(1, 0), 1, -1.0, False, (1, 0)], [(1, 0), 0, -1.0, False, (0, 0)], [(0, 0), 3, -1.0, False, (0, 0)], [(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)], [(0, 0), 1, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 0, -1.0, False, (0, 1)], [(0, 1), 3, -1.0, False, (0, 0)], [(0, 0), 2, -1.0, False, (1, 0)], [(1, 0), 2, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 1, -1.0, False, (3, 0)], [(3, 0), 2, -1.0, False, (4, 0)], [(4, 0), 0, -1.0, False, (3, 0)], [(3, 0), 2, -1.0, False, (4, 0)], [(4, 0), 0, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 0, -1.0, False, (2, 0)], [(2, 0), 3, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 3, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 2, -1.0, False, (4, 0)], [(4, 0), 0, -1.0, False, (3, 0)], [(3, 0), 1, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 0, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 0, -1.0, False, (2, 0)], [(2, 0), 3, -1.0, False, (2, 0)], [(2, 0), 1, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 1, -1.0, False, (3, 0)], [(3, 0), 2, -1.0, False, (4, 0)], [(4, 0), 0, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 0, -1.0, False, (2, 0)], [(2, 0), 2, -1.0, False, (3, 0)], [(3, 0), 1, -1.0, False, (3, 0)], [(3, 0), 3, -1.0, False, (3, 0)], [(3, 0), 2, -1.0, False, (4, 0)], [(4, 0), 1, -1.0, False, (4, 1)], [(4, 1), 3, -1.0, False, (4, 0)], [(4, 0), 1, -1.0, False, (4, 1)], [(4, 1), 1, -1.0, False, (4, 2)], [(4, 2), 1, -1.0, False, (4, 2)], [(4, 2), 1, -1.0, False, (4, 2)], [(4, 2), 2, -1.0, False, (4, 2)], [(4, 2), 3, -1.0, False, (4, 1)], [(4, 1), 0, -1.0, False, (4, 1)], [(4, 1), 2, -1.0, False, (4, 1)], [(4, 1), 1, -1.0, False, (4, 2)], [(4, 2), 0, -1.0, False, (3, 2)], [(3, 2), 3, -1.0, False, (3, 2)], [(3, 2), 3, -1.0, False, (3, 2)], [(3, 2), 3, -1.0, False, (3, 2)], [(3, 2), 0, -1.0, False, (3, 2)], [(3, 2), 3, -1.0, False, (3, 2)], [(3, 2), 3, -1.0, False, (3, 2)], [(3, 2), 2, -1.0, False, (4, 2)], [(4, 2), 3, -1.0, False, (4, 1)], [(4, 1), 1, -1.0, False, (4, 2)], [(4, 2), 1, -1.0, False, (4, 2)], [(4, 2), 0, -1.0, False, (3, 2)], [(3, 2), 0, -1.0, False, (3, 2)], [(3, 2), 2, -1.0, False, (4, 2)], [(4, 2), 0, -1.0, False, (3, 2)], [(3, 2), 1, -1.0, False, (3, 3)], [(3, 3), 1, -1.0, False, (3, 4)], [(3, 4), 2, -1.0, True, (4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "episode = []\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, extra_info = env.step(action)\n",
    "    episode.append([state, action, reward, done, next_state])\n",
    "    state = next_state\n",
    "env.close()\n",
    "\n",
    "print(f\"first episode:\\n{episode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a98eb",
   "metadata": {},
   "source": [
    "##### Recompensas y devoluciones\n",
    "\n",
    "Una recompensa es un feedback numérico que genera el entorno cuando el agente realiza una acción *a* en un estado *s*:\n",
    "\n",
    "\\begin{equation}\n",
    "    r = r(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Generemos una recompensa del entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "062e03e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We achieved a reward of -1.0 by taking action 2 in state (0, 0)\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _ = env.step(action)\n",
    "print(f\"We achieved a reward of {reward} by taking action {action} in state {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd6ae2",
   "metadata": {},
   "source": [
    "La rentabilidad asociada a un momento en el tiempo *t* es la suma (descontada) de recompensas que obtiene el agente a partir de ese momento. Vamos a calcular $G_0$, es decir, la vuelta al principio del episodio:\n",
    "\n",
    "\\begin{equation}\n",
    "    G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 + ... + \\gamma^{T-1} R_T\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "Supongamos que el factor de descuento $\\gamma = 0.99$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149ea534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took us 352 moves to find the exit, \n",
      "    and each reward r(s,a)=-1, so the return amounts to -97.0920395314196\n"
     ]
    }
   ],
   "source": [
    "env = Maze()\n",
    "state = env.reset()\n",
    "done = False\n",
    "gamma = 0.99\n",
    "G_0 = 0\n",
    "t = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _ = env.step(action)\n",
    "    G_0 += gamma ** t * reward\n",
    "    t += 1\n",
    "env.close()\n",
    "\n",
    "print(\n",
    "    f\"\"\"It took us {t} moves to find the exit, \n",
    "    and each reward r(s,a)=-1, so the return amounts to {G_0}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b4f5b",
   "metadata": {},
   "source": [
    "##### Política\n",
    "\n",
    "Una política es una función $\\pi(a|s) \\in [0, 1]$ que da la probabilidad de una acción dado el estado actual. La función toma el estado y la acción como entradas y devuelve un valor flotante en [0,1].\n",
    "\n",
    "Dado que en la práctica necesitaremos calcular las probabilidades de todas las acciones, representaremos la política como una función que toma el estado como argumento y devuelve las probabilidades asociadas con cada una de las acciones. Por lo tanto, si las probabilidades son:\n",
    "\n",
    "[0,5, 0,3, 0,1]\n",
    "\n",
    "entenderemos que la acción con índice 0 tiene un 50% de probabilidad de ser elegida, la de índice 1 tiene un 30% y la de índice 2 tiene un 10%.\n",
    "\n",
    "Codifiquemos una función de política que elija acciones al azar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af99764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    return np.array([0.25] * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943c14b",
   "metadata": {},
   "source": [
    "## Ejejcutando un episodio con una política aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409fcc0",
   "metadata": {},
   "source": [
    "###### Crear y resetear el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2579c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Maze()\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b08634",
   "metadata": {},
   "source": [
    "###### Compute $p(a|s) \\; \\forall a \\in \\{0, 1, 2, 3\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9daeaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probabilities = random_policy(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30965dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('Up', 'Right', 'Down', 'Left')\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, action_probabilities, alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('P(a|s)')\n",
    "plt.title('Random Policy')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a7a52",
   "metadata": {},
   "source": [
    "###### Usar la política para ejecutar un episodio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0c5f7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "img = plt.imshow(env.render(mode='rgb_array')) \n",
    "while not done:\n",
    "    action = np.random.choice(range(4), 1, p=action_probabilities)\n",
    "    _, _, done, _ = env.step(action)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9340bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ff32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
