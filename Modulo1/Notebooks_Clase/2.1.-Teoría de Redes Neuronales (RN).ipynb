{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 1: Teoría de Redes Neuronales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales artificiales (RNA) constituyen un paradigma de computación inspirado en las <a href=\"https://es.wikipedia.org/wiki/Neurona\">neuronas</a> biológicas y su interconexión. Las neuronas biológicas son células compuestas principalmente de tres partes: soma (cuerpo celular), dendritas (canales de entrada) y axón (canal de salida). Descrito de una forma muy simplificada, las neuronas transmiten información a través de procesos electroquímicos. Cuando una neurona recibe, a través de las denritas, una cantidad de estímulos mayor a un cierto umbral, ésta se despolariza excitando, a través del axón, a otras neuronas próximas conectadas a través de las sinapsis.\n",
    "\n",
    "<img src=\"Figures/neurona.jpg\" width=\"70%\">\n",
    "\n",
    "## La neurona artificial\n",
    "\n",
    "Inspirados por esta idea se concibió el modelo de <a href=\"https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts\">neurona artificial</a>. Fundamentalmente, consiste en una unidad de cálculo que admite como entrada un vector de características $\\vec{x}$ cuyos valores se suman de forma ponderada mediante unos pesos $\\vec{w}$ y, si esta suma supera un cierto umbral $\\theta$, genera un cierto valor de salida, por ejemplo $1$ y, si no lo supera, genera otro valor, por ejemplo, un $0$. Cuando la neurona está sola, es decir, no conectada a otras conformando una red, actúa como un clasificador lineal. \n",
    "\n",
    "<img src=\"Figures/neurona_artificial.png\" width=\"40%\">\n",
    "\n",
    "La expresión más básica de la neurona artificial es la siguiente:\n",
    "\n",
    "$$\n",
    "y=f(\\textbf{x}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  x_i} \\geq \\theta \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Perceptrón como clasificador lineal\n",
    "\n",
    "Volvamos de nuevo a la definición de neurona articial y veamos qué relación tiene con los problemas de clasificación lineal. Recordemos su expresión como la vimos arriba, pero vamos a modificarla ligeramente moviendo $\\theta$ a la izquierda del símbolo \"mayor o igual\", de esta manera:\n",
    "\n",
    "$$\n",
    "g(\\textbf{e}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  e_i} - \\theta\\geq 0 \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Si queremos, podemos visualizar gráficamente la neurona de esta manera:\n",
    "\n",
    "<img src=\"Figures/model.svg\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de activación comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sigmoide o función logística\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "This functions will squash every input to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "X = np.arange(-10, 10, 0.1)\n",
    "fig= plt.figure(figsize=(8, 6))\n",
    "plt.plot(X, sigmoid(X), label='sigmoide', c='blue', linewidth=3)\n",
    "plt.plot(X, grad_sigmoid(X), label='derivada de sigmoide', c='green', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('X')\n",
    "plt.grid(True)\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.ylabel('output')\n",
    "plt.title('Sigmoide')\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema con la función sigmoide:\n",
    "#### 1. Vanishing gradient\n",
    "En la figura anterior, cuando la salida está cerca de 0 y 1, la derivada es casi cero. Lo que significa que, durante la retropropagación, los pesos se actualizarán muy, muy lentamente, casi insignificante, por lo tanto, el aprendizaje será casi nulo. Así que los pesos iniciales no cambiarán tanto de los que se han elegido.\n",
    "\n",
    "Si se observa la derivada de sigmoide, el valor máximo es 0,25. Por lo tanto, cada gradiente se reducirá al menos al 25% de su valor (en el peor de los casos, 0) y, por lo tanto, en una red profunda, se pierde más conocimiento. Los gradientes que se propagan a las capas superficiales (capas cercanas a la capa de entrada) no serán tan significativos para actualizar los pesos.\n",
    "\n",
    "#### 2. Not zero-centered\n",
    "En el algoritmo de propagación:\n",
    "\n",
    "$$f=\\sum w_ix_i+b$$ $$\\frac{df}{dw_i}=x_i$$ $$\\frac{dL}{dw_i}=\\frac{dL}{df}\\frac{df}{dw_i}=\\frac{dL}{df}x_i$$\n",
    "\n",
    "porque $x_i>0$, el gradientet $\\dfrac{dL}{dw_i}$ siempre tiene el mismo signo $\\dfrac{dL}{df}$ (todos positivos o todos negativos).\n",
    "\n",
    "Por lo tanto, si hay un peso que debe actualizarse en positivo y otro en negativo, eso no sucederá y tardará en converger.\n",
    "\n",
    "#### 3. Computationally expensive\n",
    "El cálculo del sigmoide es computacionalmente costoso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tangente Hiperbólica\n",
    "\n",
    "$$f = tanh(x)$$\n",
    "\n",
    "La salida se limita a [-1, 1]. Esta función es casi la misma que la función sigmoide, pero está centrada en cero, por lo tanto, es mejor que la función sigmoide.\n",
    "\n",
    "Además, tanh es solo una versión escalada de la sigmoide:\n",
    "\n",
    "$$tanh(x) = 2\\sigma(2x) - 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def grad_tanh(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "X = np.arange(-10, 10, 0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, np.tanh(X), label='tanh',  c='blue', linewidth=3)\n",
    "ax.plot(X, grad_tanh(X), label='Derivative of tanh', c='green', linewidth=3)\n",
    "plt.grid(True)\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('X')\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.ylabel('output')\n",
    "plt.title('tanh(X)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rectified Linear Unit (ReLU)\n",
    "\n",
    "$$f(x) = max(0, x)$$\n",
    "\n",
    "ReLU es una de las funciones de activación más utilizadas que tiene muchas ventajas:\n",
    "\n",
    "#### 1. Resuelve el problema del vanishing gradient \n",
    "ReLU no sufre el problema del gradiente de fuga ya que las pendientes son arbitrarias con respecto a las entradas\n",
    "\n",
    "#### 2. Computacionalmente barato y simple de implementar\n",
    "ReLU no involucra ninguna operación matemática y simplemente reemplaza x < 0 con 0.\n",
    "\n",
    "### Problema con la ReLU\n",
    "\n",
    "#### Problema Dying ReLU  \n",
    "\n",
    "Debido a la naturaleza de ReLU, si un gradiente se propaga a la entrada, la red puede llegar a un estado en el que el sesgo sea muy bajo (negativo) y, por lo tanto, la salida de ReLU será 0 sin importar la entrada. Y dado que el gradiente de 0 es 0, la red no se recuperará de este estado y permanecerá siempre igual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.array([max(0, i) for i in x]) \n",
    "\n",
    "def grad_relu(x):\n",
    "    return np.array([1 if i > 0 else 0 for i in x])\n",
    "\n",
    "X = np.arange(-3, 3, 0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.plot(X, relu(X), label='ReLU',  c='blue', linewidth=3)\n",
    "ax.plot(X, grad_relu(X), label='Derivative of ReLU', c='green')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlabel('X')\n",
    "plt.ylim([-0.5, 3])\n",
    "plt.ylabel('output')\n",
    "plt.title('Rectified Linear Unit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leaky ReLU\n",
    "\n",
    "$$f_r(x) = max(\\alpha x, x)$$\n",
    "\n",
    "Leaky ReLU tiene como objetivo resolver el problema de Dying ReLU agregando un pequeño valor alfa a los valores negativos. Por tanto, el gradiente sobre los valores negativos será alpa y la red podrá recuperarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuo0lEQVR4nO3dd5gUVfb/8fdhGByQJElJAq6sYiCjooKYEBUBRSTtumBARJQVddc1ga76c11ERViSIogoiiigIqKurBEEWYwooF/CIGFEHWDJM/f3R3VPzwyTu3uqw+f1PP3Qt7q66hQ0fbrq3jrXnHOIiEjyquB3ACIi4i8lAhGRJKdEICKS5JQIRESSnBKBiEiSUyIQEUlySgQihTCz6Wb2oN9xiESbEoEkBDNbb2YX+B1HQczMmdn/zGy3mW02s7FmllLC9xZ4XIFtHp9v2Wgzez5ScUvyUCIQKR+tnHNVgXOAvsA1PscjkkOJQBKamVUwszvN7Acz22FmL5tZrVyvzzGzrWaWaWYfmNnJhWynmpm9b2bjzGyCmT2W7/XXzezPxcXjnFsHfAy0zvXe7ma2ysx+M7NPzKxlWY9XpCyUCCTR3QL0wvsl3gD4FZiQ6/W3gOZAPWAlMCv/BsysNvAe8LFz7hZgBtDfzCoEXq8DnA+8WFwwZnYi0AlYF2i3BaYBNwC1gcnAAjM7ovSHKlI2SgSS6G4A7nbOpTvn9gOjgSvNrCKAc26ac25XrtdamVmNXO9vAPwHmOOcuyfwns+ATLwvf4B+wBLn3LYi4lhpZv8DVgNLgH8Fll8PTHbOLXPOZTnnZgD7gTPCPG6RElMikETXBHgtcNnlN7wv4izgaDNLMbNHApeNdgLrA++pk+v9lwKVgUn5tjsD+EPg+R+AmcXE0Raoitc/cDpwZK74bgvGF4ixMV4CKkoWkJpvWSpwsJj3iRxGiUAS3SbgYudczVyPNOfcZmAA0BO4AKgBNA28x3K9fyqwCFhoZkfmWv480NPMWgEtgHnFBeI8LwOfAvfliu+hfPFVcc4Vd5lpY654g5oBG4qLQyQ/JQJJJKlmlpbrURHvl/xDZtYEwMzqmlnPwPrV8C7D7ACqAA8Xst3hwPfAG2ZWGcA5lw4sxzsTmOuc21uKOB8BhpjZMXiJZqiZnW6eI83sUjOrVsxxvQTcY2aNAh3iFwCXAa+UIg4RQIlAEstCYG+ux2jgSWABsNjMdgFL8S7NADyH9wt6M/Bt4LXDOG/SjiF4v97nm1la4KUZwKkUf1ko//a+wut3uMM5twKvn2A8Xkf2OmBQCY7rAeAT4KPA+x4FBjrnvi5NLCIApolpRMrGzDrjXSJq6pzL9jsekbLSGYFIGZhZKjACeFpJQOKdEoFIKZlZC+A3oD7whK/BiESALg2JiCQ5nRGIiCS5in4HUFp16tRxTZs29TsMkcN8//33AJxwwgk+RyJyuM8///xn51zdgl6Lu0TQtGlTVqxY4XcYIofp0qULAEuWLPE1DpGCmFmhNxvq0pCISJJTIhARSXJKBCIiSS7u+ggKcvDgQdLT09m3b5/foUgSSktLo1GjRn6HIVJmUUsEgXosHwBHBPbzinNuVL51DK8WzCXAHmCQc25lafeVnp5OtWrVaNq0Kd4mRcqHc44dO3aQnp7udygiZRbNS0P7gfOcc63wpuXrZmb5J9u4GG92qOZ4Rb0mlmVH+/bto3bt2koCUu7MjNq1a+tsVOJa1BJBoPb67kAzNfDIfxtzT+C5wLpLgZpmVr8s+1MSEL/osyfRlpUFQ4fC11GqLRvVzuLADFCrgO3AO865ZflWaYhX2jcoPbBMREQCbr8dJk+GM8+ERYsiv/2oJoLAHKytgUbAaWZ2Sr5VCvopdVjxIzMbYmYrzGxFRkZGFCINX9WqVcPexujRoxkzZkyZ3puSkkLr1q055ZRTuOyyy/jtt99Kva9Bgwbxyit55zWJxHGJSNlNmgRPPOE937ULPvgg8vsol+Gjzrnf8Cbs7pbvpXS8+VmDGgE/FfD+Kc659s659nXrFniHdNKrXLkyq1at4uuvv6ZWrVpMmDDB75BEJEzvvAPDh4faV1wBDz4Y+f1ELREEpgSsGXheGW9e2O/yrbYAuDowRd8ZQKZzbku0YipvP/zwA926daNdu3Z06tSJ777zDv/111/n9NNPp02bNlxwwQVs27btsPdOnTqViy++mDvuuIMnn3wyZ/ndd9/NuHHjitxvx44d2bx5c5ExiEhsW70a+vTx+gcA2rWDmTOhQhS+taN5H0F9YIaZpeAlnJedc2+Y2VAA59wkvCn4LsGbnm8PMDjcnUaz3660FbuHDBnCpEmTaN68OcuWLWPYsGH8+9//5uyzz2bp0qWYGU8//TSPPvoojz32WM77xo8fz+LFi5k3bx5btmzhiiuuYMSIEWRnZzN79mw+++yzQveZlZXFe++9x7XXXltkDCISu37+Gbp3h8xMr92wISxYAFWqRGd/UUsEzrkvgTYFLJ+U67kDbopWDH7avXs3n3zyCX369MlZtn//fsC776Fv375s2bKFAwcO0KxZs5x1Zs6cSaNGjZg3bx6pqak0bdqU2rVr89///pdt27bRpk0bateufdj+9u7dS+vWrVm/fj3t2rXjwgsvLDKGghQ0+kUjYkTK1/79cPnl8OOPXrtKFXj9dWjQIHr7TIg7i2NRdnY2NWvWZNWqVYe9dvPNNzNy5Eh69OjBkiVLGD16dM5rp5xyCqtWrSI9PT0nQVx33XVMnz6drVu3cs011xS4v2AfQWZmJt27d2fChAkMGjSo0BgKUrt2bX799dec9i+//EKdOnVKfMwiEh7n4Prr4aOPvLYZvPACtDnsJ3VkJVytIeei9yiN6tWr06xZM+bMmROIy/HFF18AkJmZScOG3ijZGTNm5HlfmzZtmDx5Mj169OCnn7x+88svv5xFixaxfPlyLrrooiL3W6NGDcaNG8eYMWOoXLlyoTEUpEuXLrz00kscOHAAgOnTp3PuueeW7sBFpMz+3//z+gGCHn0UevaM/n4TLhH4Zc+ePTRq1CjnMXbsWGbNmsUzzzxDq1atOPnkk5k/fz7gDd3s06cPnTp1KvAX99lnn82YMWO49NJL+fnnn6lUqRLnnnsuV111FSkpKcXG0qZNG1q1asXs2bMLjQHgwQcfzBNz9+7d6dSpE+3ataN169Z8/PHH/OMf/4jcX5KIFGrOHLj77lD7uuvgttvKZ99xN2dx+/btXf6JaVavXk2LFi18iij6srOzadu2LXPmzKF58+Z+hyMFWL16NTfeeCOgiWmk9JYvh86dIVip5NxzvRvHKlWK3D7M7HPnXPuCXtMZQYz79ttvOf744zn//POVBEQS0MaN0KNHKAn8/vcwd25kk0Bx1Fkc40466SR+DA4fEJGEsmsXXHYZbN3qtWvVgjfegKOOKt84dEYgIuKDrCwYMAC+/NJrp6bCq6+CHyf+SgQiIj74y1+8X/9BkyfDOef4E4sSgYhIOZsyBcaODbX/+lcYHHZdhbJTIhARKUfvvgvDhoXal18ODz/sXzygRBAxwTLQJ598Mq1atWLs2LFkZ2eXejtnnnlmmfa/fv16XnjhhZz2ihUruOWWW8q0rZLq378/LVu25PHHH8+zPJxy2rmVtQT26NGjadiwIa1bt+akk07ixRdfLPW+1q9fzymn5K2aHqnjkuT13Xdw5ZWhQnJt20avkFxpaNRQhARLPABs376dAQMGkJmZyf3331+i92dlZZGSksInn3xSpv0HE8GAAQMAaN++Pe3bFzhkOCK2bt3KJ598woYNG6K2j3Dceuut3H777axdu5Z27dpx5ZVXkpqa6ndYksTyF5Jr0MArJHfkkf7GBTojiIp69eoxZcoUxo8fj3OOrKws7rjjDjp06EDLli2ZPHky4N14dO655zJgwABOPfVUIPTLtG/fvixcuDBnm4MGDWLu3LmsX7+eTp060bZtW9q2bZuTOO68804+/PBDWrduzeOPP86SJUvo3r072dnZNG3aNM9ENccffzzbtm0jIyOD3r1706FDBzp06MDHH3982LHs27ePwYMHc+qpp9KmTRvef/99ALp27cr27dtp3bo1H374YYn+Xv75z3/m/B2MGjUqZ3mvXr1o164dJ598MlOmTDnsfT///DMdO3Zk/vz5NGvWjIMHDwKwc+dOmjZtmtMuSPPmzalSpUpODaXCYhCJpv37vbkEfvjBawcLyTWMkfkYE+6MwO6PXrVMN6rkd2Efd9xxZGdns337dubPn0+NGjVYvnw5+/fv56yzzqJr164AfPbZZ3z99dd5KpAC9OvXj5deeolLLrmEAwcO8N577zFx4kScc7zzzjukpaWxdu1a+vfvz4oVK3jkkUcYM2YMbwSGIQTvbq1QoQI9e/bktddeY/DgwSxbtoymTZty9NFHM2DAAG699VbOPvtsNm7cyEUXXcTq1avzxBGc4Oarr77iu+++o2vXrqxZs4YFCxbQvXv3Ehe0W7x4MWvXruWzzz7DOUePHj344IMP6Ny5M9OmTaNWrVrs3buXDh060Lt375wKq9u2baNHjx48+OCDXHjhhcybN48333yTXr16MXv2bHr37l3kL/2VK1fSvHlz6tWrV2QMItHiHNxwAwR/L5nBrFneZaFYkXCJIJYEy3csXryYL7/8MmcayMzMTNauXUulSpU47bTTDksCABdffDG33HIL+/fvZ9GiRXTu3JnKlSuTmZnJ8OHDWbVqFSkpKaxZs6bYOPr27csDDzzA4MGDmT17Nn379gXg3Xff5dtvv81Zb+fOnezatYtq1arlLPvoo4+4+eabATjxxBNp0qQJa9asoXr16qX6u1i8eDGLFy+mTaCM4u7du1m7di2dO3dm3LhxvPbaawBs2rSJtWvXUrt2bQ4ePMj555/PhAkTOCcwru66667j0UcfpVevXjz77LNMnTq1wP09/vjjTJ06lR9//JFFgUlei4ohv8LKb6sst5TWI49A7tqS//gH9OrlWzgFUiKIkh9//JGUlBTq1auHc46nnnrqsMqhS5Ys4chCLhCmpaXRpUsX3n77bV566SX69+8PeF9wRx99NF988QXZ2dmkpaUVG0vHjh1Zt24dGRkZzJs3j3vuuQfwahh9+umnVK5cudD3RqoWlXOOv/3tb9xwww15li9ZsoR3332XTz/9lCpVqtClSxf2Be61r1ixIu3atePtt9/OSQRnnXUW69ev5z//+Q9ZWVmHdegGBfsIXn31Va6++mp++OGHQmMoSP6S3OCV5S4oaYsUZu5cuOuuUPuaa7yJ6GNNwiWC0ly+iZaMjAyGDh3K8OHDMTMuuugiJk6cyHnnnUdqaipr1qzJKUNdlH79+vH000+zYsUKpk+fDnhnE40aNaJChQrMmDGDrMDwg2rVqrFr164Ct2NmXH755YwcOZIWLVrkXHbp2rUr48eP54477gBg1apVtG7dOs97O3fuzKxZszjvvPNYs2YNGzdu5IQTTmDLltLNKHrRRRdx7733MnDgQKpWrcrmzZtJTU0lMzOTo446iipVqvDdd9+xdOnSPHFPmzaNPn368Mgjj3DnnXcCcPXVV9O/f3/uvffeYvd7xRVXMGPGDGbMmFFoDPXq1TvsfVWrVqV+/fq89957nH/++fzyyy8sWrSIESNGlOq4JXmtWAF//GOo3aULTJwY3VkUyyrhEoFfgjOEHTx4kIoVK/LHP/6RkSNHAt7ljPXr19O2bVucc9StW5d58+YVu82uXbty9dVX06NHDyoFKlANGzaM3r17M2fOHM4999ycM4qWLVtSsWJFWrVqxaBBg3IufwT17duXDh065CQUgHHjxnHTTTfRsmVLDh06ROfOnZk0aVKe9w0bNoyhQ4dy6qmnUrFiRaZPn84RRxxRbOwPPvggTzzxRE47PT2d1atX07FjR8D7on3++efp1q0bkyZNomXLlpxwwgmcccYZebaTkpLC7Nmzueyyy6hevTrDhg1j4MCB3HPPPTlnScW57777GDBgAKtXry4whnr16uWUEQ8aOXIkzz33HDfddBO3BWoBjxo1it/97ncl2qckt02bvBpCe/d67ebNy7+QXGmoDLXEnVdeeYX58+czM/cMHj5TGWoJ2r0bOnWC4DiKo46CpUu9qqJ+KqoMtc4IJK7cfPPNvPXWW3mG1orEiqwsGDgwlAQqVvTOBPxOAsVRIpC48tRTT/kdgkih/vpX7yaxoEmTvElmYl3C3FAWb5e4JHHosycAU6fCY4+F2nfcAdde6188pRG1RGBmjc3sfTNbbWbfmNlhwy3MrIuZZZrZqsDjvrLsKy0tjR07dug/pJQ75xw7duwo0TBeSVzvvZe3kFyvXt79A/EimpeGDgG3OedWmlk14HMze8c5922+9T50znUPZ0eNGjUiPT2djIyMcDYjUiZpaWl5RhxJcvn+e6+Q3KFDXrtNG3j+ef8LyZVG1BKBc24LsCXwfJeZrQYaAvkTQdhSU1N1o4+IlLsdO+DSSyFYyqtBA6+GUCwUkiuNcslZZtYUaAMsK+Dljmb2hZm9ZWYnF/L+IWa2wsxW6Fe/iMSCAwcOLyS3YEHsFJIrjagnAjOrCswF/uyc25nv5ZVAE+dcK+ApYF5B23DOTXHOtXfOta9bt25U4xURKU6wkNwHH4SWPf88tGvnX0zhiGoiMLNUvCQwyzn3av7XnXM7nXO7A88XAqlmVieaMYmIhOvRRyHXTfo88og301i8iuaoIQOeAVY758YWss4xgfUws9MC8eyIVkwiIuF69VUIlL0CvLmG//IX/+KJhGiOGjoL+CPwlZmtCiy7CzgWwDk3CbgSuNHMDgF7gX5OY0BFJEZ9/jn84Q+hdufO3k1jsVhIrjSiOWroI6DIvx7n3HhgfLRiEBGJlPT0vIXkjj/eOzuI1UJypRFHI11FRPyxe7eXBILV12vWhDfegEBF97inRCAiUoTsbO9yUP5Cciec4GtYEaVEICJShDvvhPnzQ+2JE+G88/yLJxqUCERECvHMM/DPf4bat90G113nXzzRokQgIlKA99+HoUND7R49vInnE5ESgYhIPmvWQO/eoUJyrVvDrFmQkuJrWFGjRCAikssvv0D37vDrr167fn2vkFzVqv7GFU1KBCIiAQcOeGcCa9d67cqVvUJyiV5lXIlARASvkNyNN8KSJaFlM2dC+wKne08sSgQiIsCYMTBtWqj98MPe2UEyUCIQkaQ3b5438XzQn/6Ut7BcolMiEJGktnIlDBzoXRoC6NQJJk+O/0JypaFEICJJa/Nmr4bQnj1e+3e/8wrJHXGEv3GVNyUCEUlK//ufd5PYTz957WAhuTpJODWWEoGIJJ1gIbmVK712Sgq88gqceKK/cflFiUBEks5dd3kdxEH/+hecf75v4fhOiUBEksqzz+atGTRyJAwZ4l88sUCJQESSxpIleb/0L7vMm4g+2SkRiEhSWLs2byG5Vq3ghRcSt5BcaSgRiEjCCxaS++UXr33MMYlfSK40lAhEJKEdOABXXumVlgZIS/MKyTVu7G9csSRqicDMGpvZ+2a22sy+MbMRBaxjZjbOzNaZ2Zdm1jZa8YhI8nEOhg3zJpkJmjkTOnTwL6ZYVDGK2z4E3OacW2lm1YDPzewd59y3uda5GGgeeJwOTAz8KSIStsce86abDHroIe/sQPKK2hmBc26Lc25l4PkuYDXQMN9qPYHnnGcpUNPM6kcrJhFJHvPnw1/+EmpffTX87W/+xRPLyqWPwMyaAm2AZfleaghsytVO5/BkgZkNMbMVZrYiIyMjanGKSGL4739hwIBQIbmzz4YpU5KrkFxpRD0RmFlVYC7wZ+fczvwvF/AWd9gC56Y459o759rXrVs3GmGKSIL46ae8heSOOw5eey35CsmVRlQTgZml4iWBWc65VwtYJR3I3XffCPgpmjGJSOIKFpLbvNlr16iRvIXkSiOao4YMeAZY7ZwbW8hqC4CrA6OHzgAynXNbohWTiCSu7GyvH+Dzz712sJBcixb+xhUPojlq6Czgj8BXZrYqsOwu4FgA59wkYCFwCbAO2AMMjmI8IpLA7r7bm0sgaPx4uOAC/+KJJ1FLBM65jyi4DyD3Og64KVoxiEhymD4dHnkk1P7zn2HoUL+iiT+6s1hE4tp//pO3kNyll3oT0UvJKRGISNxatw6uuAIOHvTaLVvCiy+qkFxpKRGISFz69Vfv13+wkNzRR3uF5KpV8zeueKREICJx5+DBggvJHXusv3HFKyUCEYkrzsFNN8G//x1aNmMGnHaafzHFOyUCEYkrjz8OU6eG2n//O1x1lX/xJAIlAhGJGwsWwO23h9p/+IN3/4CER4lAROLCqlV5C8mddRY8/bQKyUWCEoGIxLwtW7xCcv/7n9du1kyF5CJJiUBEYtqePV4hufR0r129uldIToWII0eJQERiVrCQ3IoVXjslBebMgZNO8jeuRKNEICIx6957Ye7cUPupp6BrV//iSVRKBCISk2bMgIcfDrVHjIAbb/QvnkSmRCAiMefDD+H660PtSy7xJqKX6FAiEJGYsm4dXH55qJDcqafC7NkqJBdNSgQiEjN+/RW6d4cdO7y2CsmVDyUCEYkJBw9Cnz7w/fde+4gjYP58aNLE37iSgRKBiPjOORg+HN57L7Rsxgw4/XT/YkomSgQi4rsnnoApU0LtBx6Avn19CyfplCgRmNmIkiwTESmt11+H224LtQcOhHvu8S+eZFTSM4I/FbBsUATjEJEk9MUX0L9/qJDcmWeqkJwfKhb1opn1BwYAzcxsQa6XqgE7innvNKA7sN05d0oBr3cB5gP/F1j0qnPugRJHLiJxLX8huaZNvUJyaWm+hpWUikwEwCfAFqAOkPt2jl3Al8W8dzowHniuiHU+dM51L2Y7IpJg9uyBnj1h0yavHSwkV6+ev3ElqyITgXNuA7AB6FjaDTvnPjCzpmWMS0QSVHY2/OlPsHy5105JgZdfhpNP9jeuZFbSzuJdZrYz8NhnZllmtjMC++9oZl+Y2VtmVujHwMyGmNkKM1uRkZERgd2KiF/uuw9eeSXUHjcOLrrIv3ik+EtDADjn8tzXZ2a9gHCnil4JNHHO7TazS4B5QPNC9j8FmALQvn17F+Z+RcQnM2fCQw+F2jffDMOG+RePeMp0H4Fzbh5wXjg7ds7tdM7tDjxfCKSaWZ1wtikiseujj+C660Ltiy+GsWP9i0dCSnRGYGZX5GpWANoDYf0yN7NjgG3OOWdmpwW2W+RIJBGJTz/8AL16wYEDXvuUU7xCchVL9A0k0VbSf4bLcj0/BKwHehb1BjN7EegC1DGzdGAUkArgnJsEXAncaGaHgL1AP+ecLvuIJJjffstbSK5ePe8msurVfQ1LcilpH8Hg0m7YOde/mNfH4w0vFZEEdfAgXHUVfPed1z7iCJg3z7tnQGJHSUcNHWdmr5tZhpltN7P5ZnZctIMTkfjlHNxyC7zzTmjZ9OnQsdSD0SXaStpZ/ALwMlAfaADMAV6MVlAiEv/GjYNJk0Lt0aOhXz/fwpEilDQRmHNupnPuUODxPGF2FotI4nrzTRg5MtTu39+7f0BiU0k7i983szuB2XgJoC/wppnVAnDO/RKl+EQkznz5pffLPzvba3fsCNOmqZBcLCtpIghWBr8h3/Jr8BKD+gtEhK1bvRFCu3d77aZNvc5hFZKLbSVNBC2cc/tyLzCztPzLRCR57d2bt5BctWreMFEVkot9Je0j+KSEy0QkCWVnw6BB8NlnXrtCBa+Q3CmHFaCXWFTcfATHAA2BymbWBghe5asOVIlybCISJ0aP9r74g558Erp18y0cKaXiLg1dhDcTWSMgd1WQXcBdUYpJROLI88/D3/8eag8f7j0kfhQ3H8EMYIaZ9XbOzS2nmEQkTnz8MVx7bajdrRs8/rh/8UjZlLSz+JSC5gvQ1JIiyevHH/MWkjv5ZBWSi1cl/Sfbnet5Gt5cxKsjH46IxIPMTG+Y6M8/e+26db0RQjVq+BuXlE1Ji87lnq8YMxsDLChkdRFJYIcOeYXkVgd+CgYLyTVr5mtYEoYyTUyDN2JIN5GJJJlgIbnFi0PLpk2DM8/0LyYJX0knpvmKUG2hCkA94O+Fv0NEEtFTT8HEiaH2qFEwYIB/8UhklLSPoDtwFNAJqAksdM59Hq2gRCT2LFwIt94aavfr5yUCiX8lvTTUE5gJ1MGbZexZM7s5alGJSEz56qu8heTOOAOefVaF5BJFSc8IrgPOcM79D8DM/gF8CjwVrcBEJDZs2+aNENq1y2s3aaJCcommxPMRAFm52lmEyk2ISIIKFpLbuNFrV6sGb7wBRx/tb1wSWSU9I3gWWGZmrwXavYBnohKRiMQE52DwYFi2zGtXqAAvvaRCcomopPcRjDWzJcDZeGcCg51z/41mYCLir9GjvS/+oCeegIsv9isaiaYS3wzunFsJrCzp+mY2DW+00Xbn3GG/IczMgCeBS4A9wKDAPkTEZ7NmwQO5CsgMG6ZCcoksmlVBpgPjgecKef1ioHngcTowMfCnSFzasXcHW3dvpefsnn6HEpZffvGKyRGYaL5uPdh0OvR6qci3STkZdc4o2tZvG9FtRi0ROOc+MLOmRazSE3jOOeeApWZW08zqO+e2RCsmkWjZuX8n32Z8S3Z2Ngu+T4DqK78PPc0AXl/rWySSz43tb4z4NstaYiISGgKbcrXTA8sOY2ZDzGyFma3IyMgol+BESmPb7m1kBwfZi8QZPwvGFjT81BWwDOfcFGAKQPv27QtcR8RPWS40urpBtQb865J/+RhN6WVlwYMPwn8DQ0AqVvQmm2nRwt+45HBtjmkT8W36mQjSgca52o2An3yKRSQs2S50NlDjiBr0PDG++gmGD4f/zg61pz8PAy/3Lx4pX35eGloAXG2eM4BM9Q9IvMrKDp0RpFRI8TGS0hs/HiZMCLXvvRcGDvQvHil/UTsjMLMXgS5AHTNLB0bh1SnCOTcJWIg3dHQd3vDRwdGKRSTacp8RVDA/f1+VzltvwYgRoXbfvnD//f7FI/6I5qih/sW87oCborV/kfIUj4ng66+9L/5gH/fpp6uQXLKKj0+sSIzL3VmcYrF/aSh/Ibljj/UKyVWu7GtY4hMlApEIiKczgn37vEnnN2zw2lWrevMNH3OMr2GJj2L7EysSJ+Kls9g5uOYaWLrUa1eoALNnQ8uW/sYl/lIiEImAeDkjeOABePHFUHvsWLj0Uv/ikdgQu59YkTiSu48gVhPBiy96FUWDbrzRm4heJDY/sSJxJvcZQSx2Fn/6qTe3QNCFF8KTT2qEkHiUCEQiIJYvDa1f780ytn+/127RAl5+GVJTfQ1LYkhsfWJF4lSsdhbv3OkNEw3WaqxTx5tqsmZNX8OSGKNEIBIBsXhGcOiQd8PYN9947UqV4LXX4Ljj/I1LYk9sfGJF4lwsdhaPHAmLFoXazzwDZ5/tXzwSu2LjEysS52Kts3jCBHjqqVD7nnvgD3/wLx6JbUoEIhEQS5eGFi3KOyy0Tx8VkpOiKRGIREDuzmI/E8E33+QtJHfaaTBjhncHsUhh9PEQiYA8l4Z8GjW0fbs3QmjnTq/duDHMn69CclI8JQKRCPC7szhYSG79eq9dtao3TFSF5KQklAhEIsDPzmLn4NprvbuHwbsM9OKLKiQnJadEIBIBfvYR/P3v8MILofZjj3mXiERKSolAJAL8GjU0ezaMGhVq33BD3qknRUpCiUAkAvzoLF66FAYNCrUvuMC7d0CF5KS0lAhEIqC8O4s3bMhbSO7EE2HOHBWSk7JRIhCJgPLsLA4Wktu+3WvXrq1CchKeqCYCM+tmZt+b2Tozu7OA17uYWaaZrQo87otmPCLRUl6dxYcOQb9+8PXXXjs11Ssk97vfRW2XkgQqRmvDZpYCTAAuBNKB5Wa2wDn3bb5VP3TOaYyDxLXy6iy+7TZ4661Q++mnoVOnqO1OkkQ0zwhOA9Y55350zh0AZgM9o7g/Ed+Ux6Whf/0Lxo0Lte+6C66+Oiq7kiQTzUTQENiUq50eWJZfRzP7wszeMrOTC9qQmQ0xsxVmtiIjOMOGSAyJdmfx22/nLSR35ZXe/QMikRDNRFDQIDaXr70SaOKcawU8BcwraEPOuSnOufbOufZ169aNbJQiERDN4aPffgtXXQVZgVzToYMKyUlkRfOjlA40ztVuBPyUewXn3E7n3O7A84VAqpnViWJMIlERrc7ijIy8heQaNfIKyVWpErFdiEQ1ESwHmptZMzOrBPQDFuRewcyOMfNufzGz0wLx7IhiTCJREY3O4mAhuf/7P6995JHeMNH69SOyeZEcURs15Jw7ZGbDgbeBFGCac+4bMxsaeH0ScCVwo5kdAvYC/Zxz+S8ficS83H0Ekegsdg6uvx4++cRrm3mF5Fq1CnvTIoeJWiKAnMs9C/Mtm5Tr+XhgfDRjECkPkT4jeOgheP75UHvMGLjssrA3K1IgdTeJREAkO4tffhnuvTfUHjIEbr01rE2KFEmJQCQCItVZvGwZ/OlPofb558P48SokJ9GlRCASAZG4NBQsJLdvn9c+4QQVkpPyoUQgEgHhdhbv2uX1AWzb5rVr1fJGCB11VKQiFCmcEoFIBIRzRpCVBf37w1dfee1gIbnjj49khCKFUyIQiYBwOotvvx3efDPUnjoVOneOVGQixVMiEImAsnYWT5oETzwRav/tb3k7i0XKgxKBSASU5dLQO+/A8OGhdu/e8OCDkY5MpHhKBCIRUNrO4tWroU+fUCG5du3guedUSE78oY+dSASU5owgIwMuvRQyM712o0awYIEKyYl/lAhEIiB3H0FRncX798MVV+QtJPf669CgQbQjFCmcEoFIBJTkjCBYSO6jj7y2GcyaBa1bl0OAIkVQIhCJgJIkgocfhpkzQ+1HH/XuJBbxmxKBSAQU11k8Zw7cc0+ofd113kT0IrFAiUAkAoo6I/jss7yTzJ97LkyYoEJyEjuUCEQiIM8ZQa7O4o0boUePUCG53/8e5s6FSpXKO0KRwkV1YppYsWv/LkYsGuF3GJLAPtr4Uc7z4BmBCslJvEiKRLDv0D6eXfWs32FIkqhgFcjKggED4MsvvWWpqfDqq9C8ub+xiRREl4ZEIsmgc5PO3HGH9+s/aPJkOOcc/8ISKUpSnBFUrVSVZ3o843cYkuAeff1RaqbV5K3Zx/L446Hlf/0rDB7sX1wixUmKRFA5tTLXtLnG7zAkwT1X9Tl+/RVuuim07PLLvfsHRGJZVC8NmVk3M/vezNaZ2Z0FvG5mNi7w+pdm1jaa8YhE05498M03oUJybdt6N5CpkJzEuqidEZhZCjABuBBIB5ab2QLn3Le5VrsYaB54nA5MDPwpEtOysmDrVm946IYN3p9ffRVKAg0aeIXkjjzS3zhFSiKal4ZOA9Y5534EMLPZQE8gdyLoCTznnHPAUjOraWb1nXNbCtvo999/T5cuXaIYtoj3hb5/vzf+v7A/D7cKgAoVulC/PgwcWJ4Ri5RdNBNBQ2BTrnY6h//aL2idhkCeRGBmQ4AhAEcccUTEA5Xkc+BA0V/yhw6VfdstWkDVqpGLVSTaopkICrqB3pVhHZxzU4ApAO3bt3dLliwJOzhJXHv3wqZNoUs2wUewvWmTlwjCVacONGkCxx7rPd54owu1a8OyZUvC37hIhFkRNU2imQjSgca52o2An8qwjkgO57yJXfJ/ued+npER/n5SU6FxY+8LPveXfbDduPHhE8msWhX+fkX8EM1EsBxobmbNgM1AP2BAvnUWAMMD/QenA5lF9Q9I4tu/3/vFXtAXffBR8PX50qlVK+8Xe/4v+qOP1mgfSR5RSwTOuUNmNhx4G0gBpjnnvjGzoYHXJwELgUuAdcAeQLfdJDDn4JdfCv4VH3xs3Rr+fipW9KZ/zP/lHnzeuDFUqxb+fkQSRVRvKHPOLcT7ss+9bFKu5w64Kf/7JD4dOACbNxd+bX7jRm+sfbhq1Cj8l/yxx0L9+pBS/PzxIhKQFHcWS/icg99+K/yX/IYNsGWLt144KlSAhg2LvjZfo0ZEDklEApQIBPCGS27eXPQX/e7d4e+natW8X/D5nzdo4F3aEZHyo/9ySWLnzqIv2WzeDNnZxW+nKGbeZZnCvuiPPRZq1tTMXCKxRokgAWRleZdlChtOuXEjZGaGv58qVYq+Nt+woWbeEolHSgRxYPfuoi/ZbN4c3p2wQcccU/SQylq19GteJBEpEfgsOztUvKywL/pffw1/P2lphQ+nDA6pVPUOkeSkRBBle/YcfkNU7i/7TZvg4MHw91O3buGXbI491ntdv+ZFpCBKBGFwDrZvL/yX/MaN8PPP4e+nUqWCyx0EnzduDJUrh78fEUlOSgRF2LcvVO6goC/6TZu8kgjhql276Gvz9eqp3IGIRE/SJgLnvF/rhV2y2bDB+7UfrooVQ7/mCyt3oJLFIuKnpEgEW7fC1KmH/6rfuzf8bdesWfS1+WOOUbkDEYltSZEIdu+G++4r/ftSUoovd1C9euTjFREpT0mRCBo1Knh5tWpFlzuoX1/lDkQk8SXF11xaGtx/vzejVP5yByIiyS4pEgGU7dKQiEgy0KBEEZEkp0QgIpLklAhERJKcEoGISJJTIhARSXJKBCIiSU6JQEQkyZlzzu8YSsXMMoANZXx7HSAChaFjgo4lNulYYk+iHAeEdyxNnHN1C3oh7hJBOMxshXOuvd9xRIKOJTbpWGJPohwHRO9YdGlIRCTJKRGIiCS5ZEsEU/wOIIJ0LLFJxxJ7EuU4IErHklR9BCIicrhkOyMQEZF8lAhERJJc0iUCM/u7mX1pZqvMbLGZNfA7prIys3+a2XeB43nNzGr6HVNZmVkfM/vGzLLNLO6G+plZNzP73szWmdmdfscTDjObZmbbzexrv2MJh5k1NrP3zWx14LM1wu+YysrM0szsMzP7InAs90d0+8nWR2Bm1Z1zOwPPbwFOcs4N9TmsMjGzrsC/nXOHzOwfAM65v/ocVpmYWQsgG5gM3O6cW+FzSCVmZinAGuBCIB1YDvR3zn3ra2BlZGadgd3Ac865U/yOp6zMrD5Q3zm30syqAZ8DveLx38XMDDjSObfbzFKBj4ARzrmlkdh+0p0RBJNAwJFA3GZC59xi59yhQHMpUMjszLHPObfaOfe933GU0WnAOufcj865A8BsoKfPMZWZc+4D4Be/4wiXc26Lc25l4PkuYDXQ0N+oysZ5dgeaqYFHxL67ki4RAJjZQ2a2CRgIJMokltcAb/kdRJJqCGzK1U4nTr9wEpWZNQXaAMt8DqXMzCzFzFYB24F3nHMRO5aETARm9q6ZfV3AoyeAc+5u51xjYBYw3N9oi1bcsQTWuRs4hHc8MaskxxKnrIBlcXummWjMrCowF/hzvisCccU5l+Wca4135n+amUXssl1CTl7vnLughKu+ALwJjIpiOGEp7ljM7E9Ad+B8F+MdPqX4d4k36UDjXO1GwE8+xSK5BK6nzwVmOede9TueSHDO/WZmS4BuQEQ69BPyjKAoZtY8V7MH8J1fsYTLzLoBfwV6OOf2+B1PElsONDezZmZWCegHLPA5pqQX6GB9BljtnBvrdzzhMLO6wVGBZlYZuIAIfncl46ihucAJeCNUNgBDnXOb/Y2qbMxsHXAEsCOwaGkcj4C6HHgKqAv8Bqxyzl3ka1ClYGaXAE8AKcA059xD/kZUdmb2ItAFr+TxNmCUc+4ZX4MqAzM7G/gQ+Arv/zvAXc65hf5FVTZm1hKYgff5qgC87Jx7IGLbT7ZEICIieSXdpSEREclLiUBEJMkpEYiIJDklAhGRJKdEICKS5JQIRMIQqHD5f2ZWK9A+KtBu4ndsIiWlRCASBufcJmAi8Ehg0SPAFOfcBv+iEikd3UcgEqZAGYPPgWnA9UCbQBVSkbiQkLWGRMqTc+6gmd0BLAK6KglIvNGlIZHIuBjYAsTtRC6SvJQIRMJkZq3xZic7A7g1MDOWSNxQIhAJQ6DC5US8WvcbgX8CY/yNSqR0lAhEwnM9sNE5906g/S/gRDM7x8eYREpFo4ZERJKczghERJKcEoGISJJTIhARSXJKBCIiSU6JQEQkySkRiIgkOSUCEZEk9/8BZEv9iQ/oqlwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# consider alpha = 0.1\n",
    "def leaky_relu(x):\n",
    "    return np.array([max(0.1*i, i) for i in x]) \n",
    "\n",
    "def grad_leaky_relu(x):\n",
    "    return np.array([1 if i > 0 else 0.1 for i in x])\n",
    "\n",
    "X = np.arange(-3, 3, 0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, leaky_relu(X), label='Leaky ReLU',  c='blue', linewidth=3)\n",
    "ax.plot(X, grad_leaky_relu(X), label='Derivative of Leaky ReLU', c='green', linewidth=3)\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('output')\n",
    "plt.title('Leaky ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regla de aprendizaje\n",
    "\n",
    "- En las RNA se considera que el conocimiento se encuentra representado en los pesos de las conexiones.\n",
    "\n",
    "- El proceso de aprendizaje se basa en cambios en estos pesos.\n",
    "\n",
    "### Formas de conexión entre neuronas\n",
    "\n",
    "- Las salidas de las neuronas se convierten en entradas de otras neuronas.\n",
    "- Cuando ninguna salida de las neuronas es entrada de neuronas del mismo nivel o de niveles precedentes, la red se describe como propagación hacia adelante (feedforward).\n",
    "\n",
    "- En caso contrario la red se describe como propagación hacia atrás (feedback).\n",
    "\n",
    "### Características de las RNA\n",
    "\n",
    ">Topología\n",
    "\n",
    "- Número de capas.\n",
    "\n",
    "- Número de neuronas por capa.\n",
    "\n",
    "- Tipo de conexiones. Normalmente, todas las neuronas de una capa reciben señales de la capa anterior (más cercana a la entrada) y envían su salida a las neuronas de la capa posterior (más cercana a la salida de la red).\n",
    "\n",
    ">Tipos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes feedforward\n",
    "\n",
    "La más conocidas son:\n",
    "- perceptron\n",
    "- Adaline\n",
    "- Madaline\n",
    "- Backpropagation\n",
    "\n",
    "útilies en aplicaciones de reconocimiento o clasificación de patrones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo funciona una red neuronal\n",
    "\n",
    "<img src=\"Figures/red_pesos.png\" width=\"50%\">\n",
    "\n",
    "Las redes neuronales se modelan como colecciones de neuronas que están conectadas en un gráfico acíclico. En otras palabras, las salidas de algunas neuronas pueden convertirse en entradas para otras neuronas. Los ciclos no están permitidos ya que eso implicaría un bucle infinito en el paso hacia adelante de una red. Los modelos de redes neuronales a menudo se organizan en distintas capas de neuronas.\n",
    "\n",
    "Para las redes neuronales regulares, el tipo de capa más común es el **fully connected layer** en donde las neuronas entre dos capas adyacentes están completamente conectadas por pares, pero las neuronas dentro de una sola capa no comparten conexiones\n",
    "\n",
    "<img src=\"Figures/RAN_layers.png\" width=\"80%\">\n",
    "\n",
    "**Capa de salida**. A diferencia de todas las capas de una red neuronal, las neuronas comúnmente no tienen una función de activación (o se puede pensar que tienen una función de activación lineal). Esto se debe a que la última capa de salida se\n",
    "toma para representar los scores de clase (por ejemplo, en la clasificación), o pueden ser números reales (por ejemplo, en regresión)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo aprende una red neuronal\n",
    "\n",
    ">Por lo general, un modelo de red neuronal se entrena utilizando el Pesos y algoritmo de optimización de descenso de gradiente estocástico se actualizan usando el algoritmo backpropagation del error\n",
    "\n",
    ">El \"gradiente\" en el algoritmo de gradiente descendente se refiere a un gradiente de error. El modelo con un conjunto dado de pesos se utiliza para hacer predicciones y se calcula el error de esas predicciones.\n",
    "\n",
    ">El algoritmo de gradiente descendente busca cambiar los pesos para que la próxima iteración de tal forma que se reduzca el error, lo que significa que el algoritmo de optimización está navegando hacia abajo en el gradiente (o pendiente) del error.\n",
    "\n",
    "<img src=\"Figures/gradient_descent.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función de coste (loss function)\n",
    "\n",
    "\n",
    "La función de coste ( $l$ ), también llamada función de pérdida, loss function o cost function, es la encargada de cuantificar la distancia entre el valor real y el valor predicho por la red, en otras palabras, mide cuánto se equivoca la red al realizar predicciones. En la mayoría de casos, la función de coste devuelve valores positivos. Cuanto más próximo a cero es el valor de coste, mejor son las predicciones de la red (menor error), siendo cero cuando las predicciones se corresponden exactamente con el valor real.\n",
    "\n",
    "La función de coste puede calcularse para una única observación o para un conjunto de datos (normalmente promediando el valor de todas las observaciones). El segundo caso, es el que se utiliza para dirigir el entrenamiento de los modelos.\n",
    "\n",
    "Dependiendo del tipo de problema, regresión o clasificación, es necesario utilizar una función de coste u otra. En problemas de regresión, las más utilizadas son el error cuadrático medio y el error absoluto medio. En problemas de clasificación suele emplearse la función log loss, también llamada logistic loss o cross-entropy loss.\n",
    "\n",
    "\n",
    "#### Error cuadrático medio\n",
    "\n",
    "El error cuadrático medio (mean squared error, MSE) es con diferencia la función de coste más utilizada en problemas de regresión. Para una determinada observación  i , el error cuadrático se calcula como la diferencia al cuadrado entre el valor predicho  y^  y el valor real  y .\n",
    "\n",
    "$$l^{(i)}(w,b)=(\\hat{y}^{(i)}−y^{(i)})^2$$\n",
    " \n",
    "Las funciones de coste suelen escribirse con la notación  $l(w,b)$  para hacer referencia a que su valor depende de los pesos y bias del modelo, ya que son estos los que determinan el valor de las predicciones  $y(i)$ .\n",
    "\n",
    "Con frecuencia, esta función de coste se encuentra multiplicada por  1/2 , esto es simplemente por conveniencia matemática para simplificar el cálculo de su derivada.\n",
    "\n",
    "$$l^{(i)}(w,b)=\\frac{1}{2}( )^2$$\n",
    " \n",
    "Para cuantificar el error que comete el modelo todo un conjunto de datos, por ejemplo los de entrenamiento, simplemente se promedia el error de todas las  $N$  observaciones.\n",
    "\n",
    "$$L(w,b)=\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(w,b)= \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}^{(i)}−y^{(i)})^2$$\n",
    " \n",
    "Cuando un modelo se entrena utilizando el error cuadrático medio como función de coste, está aprendiendo a predecir la media de la variable respuesta.\n",
    "\n",
    "#### Error medio absoluto\n",
    "\n",
    "El error medio absoluto (mean absolute error, MAE) consiste en promediar el error absoluto de las predicciones.\n",
    "\n",
    "$$L(w,b)= \\frac{1}{n}\\sum_{i=1}^n |\\hat{y}^{(i)}−y^{(i)}|$$\n",
    " \n",
    "El error medio absoluto es más robusto frente a outliers que el error cuadrático medio. Esto significa que el entrenamiento del modelo se ve menos influenciado por datos anómalos que pueda haber en el conjunto de entrenamiento. Cuando un modelo se entrena utilizando el error absoluto medio como función de coste, está aprendiendo a predecir la mediana de la variable respuesta.\n",
    "\n",
    "#### Log loss, logistic loss o cross-entropy loss\n",
    "\n",
    "En problemas de clasificación, la capa de salida utiliza como función de activación la función softmax. Gracias a esta función, la red devuelve una serie de valores que pueden interpretarse como la probabilidad de que la observación predicha pertenezca a cada una de las posibles clases.\n",
    "\n",
    "Cuando la clasificación es de tipo binaria, donde la variable respuesta es 1 o 0, y  $p=Pr(y=1)$ , la función de coste log-likelihood se define como:\n",
    "\n",
    "$$L_{log}(y,p)=−logPr(y|p)=−(ylog(p)+(1−y)log(1−p))$$\n",
    " \n",
    "Para problemas de clasificación con más de dos clases, esta fórmula se generaliza a:\n",
    "\n",
    "$$L_{log}(Y,P)=−logPr(Y|P)=−\\frac{1}{N} \\sum_{i=0}^{N-1}  \\sum_{i=0}^{K-1} y_{i,k}log p_{i,k}$$\n",
    " \n",
    "En ambos casos, minimizar esta la función equivale a que la probabilidad predicha para la clase correcta tienda a 1, y a 0 en las demás clases.\n",
    "\n",
    "Dado que esta función se ha utilizado en campos diversos, se le conoce por nombres distintos: Log loss, logistic loss o cross-entropy loss, pero todos hacen referencia a lo mismo. Puede encontrarse una explicación más detallada de esta función de coste aquí.\n",
    "\n",
    "### Múltiples capas\n",
    "\n",
    "El modelo de red neuronal con una única capa (single-layer perceptron), aunque supuso un gran avance en el campo del machine learning, solo es capaz de aprender patrones sencillos. Para superar esta limitación, los investigadores descubrieron que, combinando múltiples capas ocultas, la red puede aprender relaciones mucho más complejas entre los predictores y la variable respuesta. A esta estructura se le conoce como perceptrón multicapa o multilayer perceptron (MLP), y puede considerarse como el primer modelo de deep learning.\n",
    "\n",
    "La estructura de un perceptón multicapa consta de varias capas de neuronas ocultas. Cada neurona está conectada a todas las neuronas de la capa anterior y a las de la capa posterior. Aunque no es estrictamente necesario, todas las neuronas que forman parte de una misma capa suelen emplear la misma función de activación.\n",
    "\n",
    "Combinando múltiples capas ocultas y funciones de activación no lineales, los modelos de redes pueden aprender prácticamente cualquier patrón. De hecho, está demostrado que, con suficientes neuronas, un MLP es un aproximador universal para cualquier función.\n",
    "\n",
    "red neuronal feed-forward (multi-layer perceptron):\n",
    "<img src=\"Figures/multilayer.png\" width=\"60%\">\n",
    "\n",
    "### Entrenamiento\n",
    "\n",
    "El proceso de entrenamiento de una red neuronal consiste en ajustar el valor de los pesos y bías de tal forma que, las predicciones que se generen, tengan el menor error posible. Gracias a esto, el modelo es capaz de identificar qué predictores tienen mayor influencia y de qué forma están relacionados entre ellos y con la variable respuesta.\n",
    "\n",
    "La idea intuitiva de cómo entrenar una red neuronal es la siguiente:\n",
    "\n",
    ">Iniciar la red con valores aleatorios de los pesos y bias.\n",
    "\n",
    ">Para cada observación de entrenamiento $( X ,  y )$, calcular el error que comete la red al hacer sus predicciones. Promediar los errores de todas las observaciones.\n",
    "\n",
    ">Identificar la responsabilidad que ha tenido cada peso y bias en el error de la predicción.\n",
    "\n",
    ">Modificar ligeramente los pesos y bias de la red (de forma proporcional a su responsabilidad en el error) en la dirección correcta para que se reduzca el error.\n",
    "\n",
    "Repetir los pasos 2, 3, 4 y 5 hasta que la red sea suficientemente buena.\n",
    "\n",
    "Si bien la idea parece sencilla, alcanzar una forma de implementarla ha requerido la combinación de múltiples métodos matemáticos, en concreto, el algoritmo de retropropagación (backpropagation) y la optimización por descenso de gradiente (gradient descent).\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "\n",
    "Backpropagation es el algoritmo que permite cuantificar la influencia que tiene cada peso y bias de la red en sus predicciones. Para conseguirlo, hace uso de la regla de la cadena (chain rule) para calcular el gradiente, que no es más que es el vector formado por las derivadas parciales de una función.\n",
    "\n",
    "En el caso de las redes, la derivada parcial del error respecto a un parámetro (peso o bias) mide cuanta \"responsabilidad\" ha tenido ese parámetro en el error cometido. Gracias a esto, se puede identificar qué pesos de la red hay que modificar para mejorarla. El siguiente paso necesario, es determinar cuánto y cómo modificarlos (optimización).\n",
    "\n",
    "\n",
    "### Descenso de gradiente\n",
    "\n",
    "\n",
    "Descenso de gradiente (gradient descent) es un algoritmo de optimización que permite minimizar una función haciendo actualizaciones de sus parámetros en la dirección del valor negativo de su gradiente. Aplicado a las redes neuronales, el descenso de gradiente permite ir actualizando los pesos y bías del modelo para reducir su error.\n",
    "\n",
    "Dado que, calcular el error del modelo para todas las observaciones de entrenamiento, en cada iteración, puede ser computacionalmente muy costoso, existe una alternativa al método de descenso de gradiente llamada gradiente estocástico (stochastic gradient descent, SGD). Este método consiste en dividir el conjunto de entrenamiento en lotes (minibatch o batch) y actualizar los parámetros de la red con cada uno. De esta forma, en lugar de esperar a evaluar todas las observaciones para actualizar los parámetros, se pueden ir actualizando de forma progresiva. Una ronda completa de iteraciones sobre todos los batch se llama época. El número de épocas con las que se entrena una red equivale al número de veces que la red ve cada ejemplo de entrenamiento.\n",
    "\n",
    "\n",
    "### Preprocesado\n",
    "\n",
    "\n",
    "A la hora de entrenar modelos basados en redes neuronales, es necesario realizar, al menos, dos tipos de transformaciones de los datos.\n",
    "\n",
    "#### Binarización (One hot ecoding) de las variables categóricas\n",
    "\n",
    "La binarización (one-hot-encoding) consiste en crear nuevas variables dummy con cada uno de los niveles de las variables cualitativas. Por ejemplo, una variable llamada color que contenga los niveles rojo, verde y azul, se convertirá en tres nuevas variables (color_rojo, color_verde, color_azul), todas con el valor 0 excepto la que coincide con la observación, que toma el valor 1.\n",
    "\n",
    "\n",
    "##### Estandarización y escalado de variables numéricas\n",
    "\n",
    "Cuando los predictores son numéricos, la escala en la que se miden, así como la magnitud de su varianza pueden influir en gran medida en el modelo. Si no se igualan de alguna forma los predictores, aquellos que se midan en una escala mayor o que tengan más varianza dominarán el modelo aunque no sean los que más relación tienen con la variable respuesta. Existen principalmente 2 estrategias para evitarlo:\n",
    "\n",
    "- Centrado: consiste en restarle a cada valor la media del predictor al que pertenece. Si los datos están almacenados en un dataframe, el centrado se consigue restándole a cada valor la media de la columna en la que se encuentra. Como resultado de esta transformación, todos los predictores pasan a tener una media de cero, es decir, los valores se centran en torno al origen.\n",
    "\n",
    "- Normalización (estandarización): consiste en transformar los datos de forma que todos los predictores estén aproximadamente en la misma escala.\n",
    "\n",
    "    - Normalización Z-score (StandardScaler): dividir cada predictor entre su desviación típica después de haber sido centrado, de esta forma, los datos pasan a tener una distribución normal.\n",
    "\n",
    "    - Estandarización max-min (MinMaxScaler): transformar los datos de forma que estén dentro del rango [0, 1].\n",
    "\n",
    "## Hiperparámetros\n",
    "\n",
    "La gran \"flexibilidad\" que tienen las redes neuronales es un arma de doble filo. Por un lado, son capaces de generar modelos que aprenden relaciones muy complejas, sin embargo, sufren fácilmente el problema de sobreajuste (**overfitting**) lo que los incapacita al tratar de prdecir nuevas observaciones. La forma de minimizar este problema y conseguir modelos útiles, pasa por configurar de forma adecuada sus hiperparámetros. Algunos de los más importantes son:\n",
    "\n",
    "### Número y tamaño de capas\n",
    "\n",
    "La arquitectura de una red, el número de capas y el número de neuronas que forman parte de cada capa, determinan en gran medida la complejidad del modelo y con ello su potencial capacidad de aprendizaje.\n",
    "\n",
    "La capa de entrada y salida son sencillas de establecer. La capa de entrada tiene tantas neuronas como predictores y la capa de salida tiene una neurona en problemas de regresión y tantas como clases en problemas de clasificación. En la mayoría de implementaciones, estos valores se establecen automáticamente en función del conjunto de entrenamiento. El usuario suele especificar únicamente el número de capas intermedias (ocultas) y el tamaño de las mismas.\n",
    "\n",
    "Cuantas más neuronas y capas, mayor la complejidad de las relaciones que puede aprender el modelo. Sin embargo, dado que en cada neurona está conectada por pesos al resto de neuronas de las capas adyacentes, el número de parámetros a aprender aumenta y con ello el tiempo de entrenamiento.\n",
    "\n",
    "\n",
    "### Learning rate\n",
    "\n",
    "\n",
    "El learning rate o ratio de aprendizaje establece cómo de rápido pueden cambiar los parámetros de un modelo a medida que se optimiza (aprende). Este hiperparámetro es uno de los más complicados de establecer, ya que depende mucho de los datos e interacciona con el resto de hiperparámetros. Si el learning rate es muy grande, el proceso de optimización puede ir saltando de una región a otra sin que el modelo sea capaz de aprender. Si por el contrario, el learning rate es muy pequeño, el proceso de entrenamiento puede tardar demasiado y no llegar a completarse.Algunas de las recomendaciones heurísticas basadas en prueba y error son:\n",
    "\n",
    "Utilizar un learning rate lo más pequeño posible siempre y cuando el tiempo de entrenamiento no supere las limitaciones temporales disponibles.\n",
    "\n",
    "No utilizar un valor constante de learning rate durante todo el proceso de entrenamiento. Por lo general, utilizar valores mayores al inicio y pequeños al final.\n",
    "\n",
    "\n",
    "### Algoritmo de optimización\n",
    "\n",
    "\n",
    "El descenso de gradiente y el descenso de gradiente estocástico fueron de los primeros métodos de optimización utilizados para entrenar modelos de redes neuronales. Ambos utilizan directamente el gradiente para dirigir la optimización. Pronto se vio que esto genera problemas a medida que las redes aumentan de tamaño (neuronas y capas). En muchas regiones del espacio de búsqueda, el gradiente es muy proximo a cero, lo que hace que la optimización quede estancada en estas regiones. Para evitar este problema, se han desarrollado modificaciones del descenso de gradiente capaces de adaptar el learning rate en función del gradiente y subgradiente. De esta forma, el proceso de aprendizaje se ralentiza o acelera dependiendo de las características de la región del espacio de búsqueda en el que se encuentren. Aunque existen multitud de adaptaciones, suele recomendarse:\n",
    "\n",
    ">Para conjuntos de datos pequeños: l-bfgs\n",
    "\n",
    ">Para conjuntos de datos grandes: adam o rmsprop\n",
    "\n",
    "La elección del algoritmo de optimización puede tener un impacto muy grande en el aprendizaje de los modelos, sobretodo en deep learning. Puede encontrarse una excelente descripción más detallada en el libro gratuito [Dive into Deep Learning](http://d2l.ai/chapter_optimization/index.html).\n",
    "\n",
    "\n",
    "### Regularización\n",
    "\n",
    "\n",
    "Los métodos de regularización se utilizan con el objetivo de reducir el sobreajuste (overfitting) de los modelos. Un modelo con sobreajuste memoriza los datos de entrenamiento pero es incapaz de predecir correctamente nuevas observaciones.\n",
    "\n",
    "Los modelos de redes neuronales pueden considerarse como modelos sobre parametrizados, por lo tanto, las estrategias de regularización son fundamentales. De entre las muchas que existen, destacan la regularización L1 y L2 (weight decay), y el dropout.\n",
    "\n",
    "#### Penalización L1 y L2\n",
    "\n",
    "El objetivo de la penalización L1 y L2, esta última también conocida como weight decay, es evitar que los pesos tomen valores excesivamente elevados. De esta forma se evita que unas pocas neuronas dominen el comportamiento de la red y se fuerza a que las características poco informativas (ruido) tengan pesos próximos o iguales a cero.\n",
    "\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "El proceso consiste en de desactivar aleatoriamente una serie de neuronas durante el proceso de entrenamiento. En concreto, durante cada iteración del entrenamiento, se ponen a cero los pesos de una fracción aleatoria de neuronas por capa. El método de dropout, descrito por Srivastava et al. en 2014, se ha convertido en un estándar para entrenar redes neuronales. El porcentaje de neuronas que suele desactivarse por capa (dropout rate) suele ser un valor entre 0.2 y 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "El \"perceptrón\" es un algoritmo simple que, dado un vector de entrada $x$ de m valores $(x_1, x_2,..., x_m)$, a menudo llamado características de entrada o simplemente características, genera un $1$ (\"sí \") o un $0$ (\"no\"). Matemáticamente, definimos una función:\n",
    "\n",
    "$$\n",
    "f(\\textbf{x}) = \\begin{cases} \\textrm{1, si} \\sum_{i=1}^{n} {w_i  x_i + b} \\geq \\theta \\\\ \\\\ 0, \\textrm{en caso contrario} \\end{cases}\n",
    "$$\n",
    "\n",
    "Donde $w=(w_1,w_2,...,w_n)$ es un vector de pesos, $wx \\sum_{i=1}^{n} {w_i x_i }$ es el producto escalar y $b$ es el sesgo . Si recuerda la geometría elemental, wx + b define un hiperplano límite que cambia de posición según los valores asignados a w y b.\n",
    "\n",
    "Tenga en cuenta que un hiperplano es un subespacio cuya dimensión es uno menos que la de su espacio ambiental.\n",
    "\n",
    "<img src=\"Figures/hiperplane.png\" width=\"40%\">\n",
    "\n",
    "En otras palabras, ¡este es un algoritmo muy simple pero efectivo! Por ejemplo, dadas tres características de entrada, las cantidades de rojo, verde y azul en un color, el perceptrón podría intentar decidir si el color es blanco o no.\n",
    "\n",
    "Tenga en cuenta que el perceptrón no puede expresar una respuesta \"tal vez\". Puede responder \"sí\" (1) o \"no\" (0), si entendemos cómo definir w y b. Este es el proceso de \"entrenamiento\" que se discutirá en las siguientes secciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RN con TensorFlow 2.0\n",
    "\n",
    "Hay cuatro partes principales:\n",
    "\n",
    ">Construcción del modelo： `tf.keras.Model` con `tf.keras.layers`\n",
    "\n",
    ">Función de pérdida de modelo： `tf.keras.losses`\n",
    "\n",
    ">Optimizador de modelos： `tf.keras.optimizer`\n",
    "\n",
    ">Evaluación del modelo： `tf.keras.metrics`\n",
    "\n",
    "Hay tres formas de crear un modelo en `tf.keras`: API secuencial, API funcional y subclases de modelos. Aquí, usaremos el más simple, `Sequential()`.\n",
    "\n",
    "El siguiente fragmento de código define una sola capa con 10 neuronas artificiales que espera 784 variables de entrada (también conocidas como características). Hay que tener en cuenta que la red es \"densa\", lo que significa que cada neurona en una capa está conectada a todas las neuronas ubicadas en la capa anterior y a todas las neuronas en la capa siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "NB_CLASSES = 10\n",
    "RESHAPED = 784\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "       input_shape=(RESHAPED,), kernel_initializer='zeros',\n",
    "       name='dense_layer', activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada neurona se puede inicializar con pesos específicos a través del parámetro `kernel_initializer`. Hay algunas opciones, las más comunes se enumeran a continuación:\n",
    "\n",
    ">`random_uniform`: Los pesos se inicializan en pequeños valores uniformemente aleatorios en el rango de -0,05 a 0,05.\n",
    "\n",
    ">`random_normal`: Los pesos se inicializan de acuerdo con una distribución gaussiana, con media cero y una pequeña desviación estándar de 0,05. Para aquellos de ustedes que no están familiarizados con la distribución gaussiana, piensen en una forma de \"curva de campana\" simétrica.\n",
    "\n",
    ">`zero`: Todos los pesos se inicializan a cero.\n",
    "\n",
    "Una lista completa de como inicializar está en: https://www.tensorflow.org/api_docs/python/tf/keras/initializers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrón Multicapa (reconocimiento de dígitos escritos a mano)\n",
    "\n",
    "En esta sección construiremos una red que puede reconocer números escritos a mano. Para lograr este objetivo, utilizaremos MNIST (http://yann.lecun.com/exdb/mnist/), una base de datos de dígitos escritos a mano compuesta por un conjunto de entrenamiento de 60 000 ejemplos y un conjunto de prueba de 10 000 ejemplos Los ejemplos de entrenamiento son anotados por humanos con la respuesta correcta. Por ejemplo, si el dígito escrito a mano es el número \"3\", entonces 3 es simplemente la etiqueta asociada con ese ejemplo.\n",
    "\n",
    "En el aprendizaje automático, cuando se dispone de un conjunto de datos con respuestas correctas, decimos que podemos realizar una forma de aprendizaje supervisado. En este caso podemos usar ejemplos de entrenamiento para mejorar nuestra red. Los ejemplos de prueba también tienen la respuesta correcta asociada a cada dígito. En este caso, sin embargo, la idea es fingir que la etiqueta es desconocida, dejar que la red haga la predicción y luego reconsiderar la etiqueta para evaluar qué tan bien nuestra red neuronal ha aprendido a reconocer dígitos. \n",
    "\n",
    "\n",
    "### One-hot encoding (OHE)\n",
    "\n",
    "Vamos a usar OHE como una herramienta simple para codificar información utilizada dentro de redes neuronales. En muchas aplicaciones es conveniente transformar características categóricas (no numéricas) en variables numéricas. Por ejemplo, la característica categórica \"dígito\" con valor d en [0 – 9] se puede codificar en un vector binario con 10 posiciones, que siempre tiene valor 0 excepto la posición d-ésima donde está presente un 1.\n",
    "\n",
    "Por ejemplo, el dígito 3 se puede codificar como [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. Este tipo de representación se denomina codificación one-hot, o a veces simplemente one-hot, y es muy común en la minería de datos cuando el algoritmo de aprendizaje se especializa en el manejo de funciones numéricas.\n",
    "\n",
    "\n",
    "### Definición de una red neuronal simple en TensorFlow 2.0\n",
    "\n",
    "Usaremos TensorFlow 2.0 para definir una red que reconozca los dígitos escritos a mano del MNIST. Empezamos con una red neuronal muy simple y luego la mejoramos progresivamente.\n",
    "\n",
    "Siguiendo el estilo de Keras, TensorFlow 2.0 proporciona [bibliotecas adecuadas](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) para cargar el conjunto de datos y dividirlo en conjuntos de entrenamiento, `X_train`, que se usan para el entrenamiento de nuestra red y conjuntos de prueba, `X_test`, utilizados para evaluar el rendimiento. Los datos se convierten en `float32` para usar una precisión de 32 bits al entrenar una red neuronal y se normalizan al rango [0,1]. Además, cargamos las etiquetas verdaderas en `Y_train` y `Y_test` respectivamente, y realizamos una codificación one-hot en ellas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# Network and training parameters.\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitivamente, `EPOCH` define cuánto debe durar el entrenamiento, `BATCH_SIZE` es la cantidad de muestras que alimenta a su red por cada iteración, y `VALIDATION` es la cantidad de datos reservados para verificar o probar la validez del entrenamiento. Veamos nuestro primer fragmento de código de una red neuronal en TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading MNIST dataset.\n",
    "# verify\n",
    "# You can verify that the split between train and test is 60,000, and 10,000 respectively. \n",
    "# Labels have one-hot representation.is automatically applied\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we  --> reshape it to \n",
    "# 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representation of the labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Se puede ver en el código anterior que la capa de entrada tiene una neurona asociada a cada píxel de la imagen para un total de **28*28=784 neuronas**, una para cada píxel en las imágenes MNIST.\n",
    "\n",
    "Normalmente, los valores asociados con cada píxel se normalizan en el rango [0,1] (lo que significa que la intensidad de cada píxel se divide por 255, el valor máximo de intensidad). La salida puede ser una de diez clases, con una clase para cada dígito.\n",
    "\n",
    "La capa final es una sola neurona con función de activación \"softmax\", que es una generalización de la función sigmoidea. Como se mencionó anteriormente, la salida de una función sigmoidea está en el rango (0, 1). De manera similar, un softmax \"aplasta\" un vector K-dimensional de valores reales arbitrarios en un vector K-dimensional de valores reales en el rango (0, 1), de modo que todos suman 1. En nuestro caso, agrega 10 respuestas proporcionadas por la capa anterior con 10 neuronas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "   input_shape=(RESHAPED,),\n",
    "   name='dense_layer', \n",
    "   activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que definimos el modelo, tenemos que compilarlo para que pueda ser ejecutado por `TensorFlow 2.0`. Hay algunas elecciones que se deben hacer durante la compilación. En primer lugar, debemos seleccionar un `optimizador`, que es el algoritmo específico que se utiliza para actualizar los pesos mientras entrenamos nuestro modelo. En segundo lugar, debemos seleccionar una función objetivo, que utiliza el optimizador para navegar por el espacio de ponderaciones (con frecuencia, las funciones objetivo se denominan funciones de pérdida o funciones de costo y el proceso de optimización se define como un proceso de minimización de pérdidas). Tercero, necesitamos evaluar el modelo entrenado. (Puede encontrar una lista completa de optimizadores en https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).\n",
    "\n",
    "Algunas opciones comunes para funciones objetivas son:\n",
    "\n",
    ">`MSE`, define el error cuadrático medio entre las predicciones y los valores verdaderos. Matemáticamente, si $d$ es un vector de predicciones, $y$ es el vector de $n$ valores observados, entonces $MSE=\\frac{1}{n}\\sum_{i=1}^n(d-y)^2$ . Hay que tener en cuenta que esta función objetivo es el promedio de todos los errores cometidos en cada predicción. Si una predicción está muy alejada del valor real, entonces esta distancia se hace más evidente mediante la operación de elevar al cuadrado. Además, el cuadrado puede sumar el error sin importar si un valor dado es positivo o negativo.\n",
    "\n",
    ">`binary_crossentropy`, define la pérdida logarítmica binaria. Supongamos que nuestro modelo predice $p$ mientras que el objetivo es $c$, entonces la entropía cruzada binaria se define como $\\mathcal{L}=-c \\ln(p) - (1-c)\\ln(1-p)$. Hay que tener en cuenta que esta función objetivo es adecuada para la predicción de etiquetas binarias.\n",
    "\n",
    ">`categorical_crossentropy`, define la pérdida logarítmica multiclase. La entropía cruzada categórica compara la distribución de las predicciones con la distribución verdadera, con la probabilidad de la clase verdadera establecida en 1 y 0 para las otras clases. Si la clase verdadera es $c$ y la predicción es $y$, entonces la entropía cruzada categórica se define como: $\\mathcal{L(c,p)} = -\\sum_i C_i \\ln(p_i)$\n",
    "\n",
    "One way to think about multi-class logarithm loss is to consider the true class represented as a one-hot encoded vector, and the closer the model's outputs are to that vector, the lower the loss. Note that this objective function is suitable for multi-class label predictions. It is also the default choice in association with softmax activation. A complete list of loss functions can be found at https://www.tensorflow.org/api_docs/python/tf/keras/losses.\n",
    "\n",
    "Some common choices for metrics are:\n",
    "\n",
    ">`Accuracy`, which defines the proportion of correct predictions with respect to the targets\n",
    "\n",
    ">`Precision`, which defines how many selected items are relevant for a multi-label classification\n",
    "\n",
    ">`Recall`, which defines how many selected items are relevant for a multi-label classification\n",
    "\n",
    "A complete list of metrics can be found at https://www.tensorflow.org/api_docs/python/tf/keras/metrics.\n",
    "\n",
    "Metrics are similar to objective functions, with the only difference that they are not used for training a model, but only for evaluating the model. However, it is important to understand the difference between metrics and objective functions. As discussed, the loss function is used to optimize your network. This is the function minimized by the selected optimizer. Instead, a metric is used to judge the performance of your network. This is only for you to run an evaluation on and it should be separated from the optimization process. On some occasions, it would be ideal to directly optimize for a specific metric. However, some metrics are not differentiable with respect to their inputs, which precludes them from being used directly.\n",
    "\n",
    "When compiling a model in TensorFlow 2.0, it is possible to select the optimizer, the loss function, and the metric used together with a given model:\n",
    "\n",
    "Una forma de pensar en la pérdida logarítmica multiclase es considerar la verdadera clase representada como un vector codificado one-hot, y cuanto más cerca estén las salidas del modelo de ese vector, menor será la pérdida. Tenga en cuenta que esta función objetivo es adecuada para predicciones de etiquetas de varias clases. También es la opción predeterminada en asociación con la activación de softmax. Puede encontrar una lista completa de funciones de pérdida en https://www.tensorflow.org/api_docs/python/tf/keras/losses.\n",
    "\n",
    "Algunas opciones comunes para las métricas son:\n",
    "\n",
    ">`Accuracy`\n",
    "\n",
    ">`Precisión`\n",
    "\n",
    ">`Recall`\n",
    "\n",
    "Puede encontrar una lista completa de métricas en https://www.tensorflow.org/api_docs/python/tf/keras/metrics.\n",
    "\n",
    "Las métricas sirven para evaluar el modelo. Es importante comprender la diferencia entre métricas y funciones objetivo. Como se discutió, la función de pérdida se utiliza para optimizar la red. Esta es la función minimizada por el optimizador seleccionado. En su lugar, se utiliza una métrica para juzgar el rendimiento de la red. Esto es solo para que ejecute una evaluación y debe estar separado del proceso de optimización. En algunas ocasiones, sería ideal optimizar directamente para una métrica específica. Sin embargo, algunas métricas no son diferenciables con respecto a sus entradas, lo que impide que se utilicen directamente.\n",
    "\n",
    "Al compilar un modelo en TensorFlow 2.0, es posible seleccionar el optimizador, la función de pérdida y la métrica utilizada junto con un modelo determinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**epochs** es la cantidad de veces que el modelo se expone al conjunto de entrenamiento. En cada iteración, el optimizador intenta ajustar los pesos para que la función objetivo se minimice.\n",
    "\n",
    ">**batch_size** es el número de instancias de entrenamiento observadas antes de que el optimizador realice una actualización de peso; suele haber muchos lotes por época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar un modelo en TensorFlow 2.0 es muy simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 1.4038 - accuracy: 0.6573 - val_loss: 0.9043 - val_accuracy: 0.8217\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.7953 - accuracy: 0.8301 - val_loss: 0.6614 - val_accuracy: 0.8548\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 0s 981us/step - loss: 0.6433 - accuracy: 0.8527 - val_loss: 0.5648 - val_accuracy: 0.8672\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.8627 - val_loss: 0.5114 - val_accuracy: 0.8763\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.5264 - accuracy: 0.8688 - val_loss: 0.4770 - val_accuracy: 0.8817\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.8739 - val_loss: 0.4528 - val_accuracy: 0.8863\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.8774 - val_loss: 0.4346 - val_accuracy: 0.8894\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.8807 - val_loss: 0.4203 - val_accuracy: 0.8921\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 0s 968us/step - loss: 0.4426 - accuracy: 0.8832 - val_loss: 0.4087 - val_accuracy: 0.8938\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4310 - accuracy: 0.8851 - val_loss: 0.3990 - val_accuracy: 0.8953\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 0s 973us/step - loss: 0.4213 - accuracy: 0.8871 - val_loss: 0.3908 - val_accuracy: 0.8971\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4129 - accuracy: 0.8887 - val_loss: 0.3839 - val_accuracy: 0.8986\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.4056 - accuracy: 0.8901 - val_loss: 0.3777 - val_accuracy: 0.8994\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 0s 985us/step - loss: 0.3991 - accuracy: 0.8913 - val_loss: 0.3724 - val_accuracy: 0.9010\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3933 - accuracy: 0.8928 - val_loss: 0.3676 - val_accuracy: 0.9015\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8940 - val_loss: 0.3632 - val_accuracy: 0.9024\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 0s 984us/step - loss: 0.3835 - accuracy: 0.8951 - val_loss: 0.3594 - val_accuracy: 0.9040\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.3793 - accuracy: 0.8958 - val_loss: 0.3556 - val_accuracy: 0.9047\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.3754 - accuracy: 0.8973 - val_loss: 0.3524 - val_accuracy: 0.9055\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8980 - val_loss: 0.3495 - val_accuracy: 0.9062\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 0s 997us/step - loss: 0.3685 - accuracy: 0.8989 - val_loss: 0.3467 - val_accuracy: 0.9072\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8998 - val_loss: 0.3440 - val_accuracy: 0.9069\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.9007 - val_loss: 0.3419 - val_accuracy: 0.9071\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 0s 997us/step - loss: 0.3598 - accuracy: 0.9011 - val_loss: 0.3395 - val_accuracy: 0.9081\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.9019 - val_loss: 0.3374 - val_accuracy: 0.9086\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 0s 993us/step - loss: 0.3549 - accuracy: 0.9024 - val_loss: 0.3354 - val_accuracy: 0.9085\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.9029 - val_loss: 0.3336 - val_accuracy: 0.9086\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 0s 984us/step - loss: 0.3505 - accuracy: 0.9033 - val_loss: 0.3320 - val_accuracy: 0.9092\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 0s 968us/step - loss: 0.3485 - accuracy: 0.9039 - val_loss: 0.3302 - val_accuracy: 0.9086\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 0s 976us/step - loss: 0.3466 - accuracy: 0.9042 - val_loss: 0.3287 - val_accuracy: 0.9095\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.9048 - val_loss: 0.3271 - val_accuracy: 0.9093\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 0s 949us/step - loss: 0.3431 - accuracy: 0.9050 - val_loss: 0.3257 - val_accuracy: 0.9098\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 0s 972us/step - loss: 0.3414 - accuracy: 0.9054 - val_loss: 0.3244 - val_accuracy: 0.9099\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 0s 934us/step - loss: 0.3399 - accuracy: 0.9055 - val_loss: 0.3230 - val_accuracy: 0.9104\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 0s 929us/step - loss: 0.3383 - accuracy: 0.9062 - val_loss: 0.3219 - val_accuracy: 0.9108\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3369 - accuracy: 0.9061 - val_loss: 0.3207 - val_accuracy: 0.9121\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.9066 - val_loss: 0.3194 - val_accuracy: 0.9108\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 0s 910us/step - loss: 0.3342 - accuracy: 0.9070 - val_loss: 0.3184 - val_accuracy: 0.9115\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 0s 960us/step - loss: 0.3329 - accuracy: 0.9074 - val_loss: 0.3174 - val_accuracy: 0.9116\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 0s 963us/step - loss: 0.3316 - accuracy: 0.9078 - val_loss: 0.3164 - val_accuracy: 0.9115\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.9078 - val_loss: 0.3156 - val_accuracy: 0.9126\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 0s 917us/step - loss: 0.3294 - accuracy: 0.9085 - val_loss: 0.3146 - val_accuracy: 0.9131\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 0s 916us/step - loss: 0.3283 - accuracy: 0.9085 - val_loss: 0.3136 - val_accuracy: 0.9127\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 0s 946us/step - loss: 0.3272 - accuracy: 0.9094 - val_loss: 0.3128 - val_accuracy: 0.9132\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3262 - accuracy: 0.9089 - val_loss: 0.3119 - val_accuracy: 0.9135\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 0s 925us/step - loss: 0.3251 - accuracy: 0.9095 - val_loss: 0.3112 - val_accuracy: 0.9131\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.9102 - val_loss: 0.3104 - val_accuracy: 0.9136\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.3232 - accuracy: 0.9104 - val_loss: 0.3097 - val_accuracy: 0.9134\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3223 - accuracy: 0.9104 - val_loss: 0.3089 - val_accuracy: 0.9138\n",
      "Epoch 50/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.9107 - val_loss: 0.3082 - val_accuracy: 0.9143\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 0s 923us/step - loss: 0.3206 - accuracy: 0.9108 - val_loss: 0.3076 - val_accuracy: 0.9141\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 0s 949us/step - loss: 0.3198 - accuracy: 0.9112 - val_loss: 0.3070 - val_accuracy: 0.9146\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.9112 - val_loss: 0.3063 - val_accuracy: 0.9144\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 0s 939us/step - loss: 0.3182 - accuracy: 0.9121 - val_loss: 0.3056 - val_accuracy: 0.9142\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.9122 - val_loss: 0.3051 - val_accuracy: 0.9143\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 0s 992us/step - loss: 0.3167 - accuracy: 0.9119 - val_loss: 0.3044 - val_accuracy: 0.9149\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 982us/step - loss: 0.3159 - accuracy: 0.9123 - val_loss: 0.3038 - val_accuracy: 0.9150\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.3152 - accuracy: 0.9123 - val_loss: 0.3033 - val_accuracy: 0.9153\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.9128 - val_loss: 0.3028 - val_accuracy: 0.9154\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.9131 - val_loss: 0.3023 - val_accuracy: 0.9152\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.9130 - val_loss: 0.3017 - val_accuracy: 0.9156\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.9132 - val_loss: 0.3012 - val_accuracy: 0.9158\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.9134 - val_loss: 0.3008 - val_accuracy: 0.9158\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 0s 976us/step - loss: 0.3113 - accuracy: 0.9136 - val_loss: 0.3003 - val_accuracy: 0.9159\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3107 - accuracy: 0.9136 - val_loss: 0.2999 - val_accuracy: 0.9161\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.9140 - val_loss: 0.2994 - val_accuracy: 0.9160\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.9143 - val_loss: 0.2990 - val_accuracy: 0.9165\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3090 - accuracy: 0.9144 - val_loss: 0.2985 - val_accuracy: 0.9162\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.9145 - val_loss: 0.2980 - val_accuracy: 0.9169\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.9145 - val_loss: 0.2977 - val_accuracy: 0.9169\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3074 - accuracy: 0.9145 - val_loss: 0.2973 - val_accuracy: 0.9173\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.9149 - val_loss: 0.2969 - val_accuracy: 0.9174\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.9149 - val_loss: 0.2965 - val_accuracy: 0.9174\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.9152 - val_loss: 0.2961 - val_accuracy: 0.9175\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 0s 983us/step - loss: 0.3054 - accuracy: 0.9154 - val_loss: 0.2959 - val_accuracy: 0.9169\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.9150 - val_loss: 0.2955 - val_accuracy: 0.9169\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3044 - accuracy: 0.9156 - val_loss: 0.2950 - val_accuracy: 0.9175\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.9153 - val_loss: 0.2947 - val_accuracy: 0.9179\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.9157 - val_loss: 0.2944 - val_accuracy: 0.9175\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3030 - accuracy: 0.9157 - val_loss: 0.2941 - val_accuracy: 0.9183\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3027 - accuracy: 0.9161 - val_loss: 0.2937 - val_accuracy: 0.9178\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.9161 - val_loss: 0.2934 - val_accuracy: 0.9181\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 0s 957us/step - loss: 0.3018 - accuracy: 0.9158 - val_loss: 0.2931 - val_accuracy: 0.9182\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.9163 - val_loss: 0.2928 - val_accuracy: 0.9184\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.9161 - val_loss: 0.2926 - val_accuracy: 0.9180\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 0s 987us/step - loss: 0.3006 - accuracy: 0.9165 - val_loss: 0.2923 - val_accuracy: 0.9186\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.9163 - val_loss: 0.2920 - val_accuracy: 0.9185\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2998 - accuracy: 0.9165 - val_loss: 0.2917 - val_accuracy: 0.9183\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 0s 997us/step - loss: 0.2994 - accuracy: 0.9168 - val_loss: 0.2913 - val_accuracy: 0.9182\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 0s 989us/step - loss: 0.2991 - accuracy: 0.9168 - val_loss: 0.2911 - val_accuracy: 0.9189\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 0s 995us/step - loss: 0.2987 - accuracy: 0.9171 - val_loss: 0.2909 - val_accuracy: 0.9185\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 0s 990us/step - loss: 0.2983 - accuracy: 0.9171 - val_loss: 0.2906 - val_accuracy: 0.9187\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.9171 - val_loss: 0.2903 - val_accuracy: 0.9188\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.9171 - val_loss: 0.2901 - val_accuracy: 0.9189\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 0s 979us/step - loss: 0.2973 - accuracy: 0.9171 - val_loss: 0.2898 - val_accuracy: 0.9189\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 0s 976us/step - loss: 0.2970 - accuracy: 0.9174 - val_loss: 0.2896 - val_accuracy: 0.9189\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2966 - accuracy: 0.9171 - val_loss: 0.2895 - val_accuracy: 0.9190\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 0s 971us/step - loss: 0.2963 - accuracy: 0.9176 - val_loss: 0.2892 - val_accuracy: 0.9191\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 0s 952us/step - loss: 0.2960 - accuracy: 0.9174 - val_loss: 0.2888 - val_accuracy: 0.9195\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 0s 925us/step - loss: 0.2956 - accuracy: 0.9179 - val_loss: 0.2887 - val_accuracy: 0.9189\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 0s 907us/step - loss: 0.2953 - accuracy: 0.9176 - val_loss: 0.2885 - val_accuracy: 0.9190\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.9176 - val_loss: 0.2883 - val_accuracy: 0.9194\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 0s 956us/step - loss: 0.2947 - accuracy: 0.9181 - val_loss: 0.2880 - val_accuracy: 0.9195\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 0s 995us/step - loss: 0.2944 - accuracy: 0.9183 - val_loss: 0.2878 - val_accuracy: 0.9197\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 0s 987us/step - loss: 0.2941 - accuracy: 0.9182 - val_loss: 0.2876 - val_accuracy: 0.9196\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 0s 943us/step - loss: 0.2938 - accuracy: 0.9180 - val_loss: 0.2873 - val_accuracy: 0.9195\n",
      "Epoch 107/200\n",
      "375/375 [==============================] - 0s 923us/step - loss: 0.2935 - accuracy: 0.9183 - val_loss: 0.2871 - val_accuracy: 0.9199\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 0s 981us/step - loss: 0.2932 - accuracy: 0.9182 - val_loss: 0.2869 - val_accuracy: 0.9196\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.9185 - val_loss: 0.2868 - val_accuracy: 0.9198\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2927 - accuracy: 0.9185 - val_loss: 0.2865 - val_accuracy: 0.9197\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.9186 - val_loss: 0.2864 - val_accuracy: 0.9201\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.9187 - val_loss: 0.2861 - val_accuracy: 0.9206\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.9191 - val_loss: 0.2859 - val_accuracy: 0.9201\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.9186 - val_loss: 0.2857 - val_accuracy: 0.9201\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.9187 - val_loss: 0.2856 - val_accuracy: 0.9202\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.9190 - val_loss: 0.2855 - val_accuracy: 0.9203\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.9188 - val_loss: 0.2853 - val_accuracy: 0.9199\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.9190 - val_loss: 0.2851 - val_accuracy: 0.9206\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.9192 - val_loss: 0.2849 - val_accuracy: 0.9207\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.9192 - val_loss: 0.2847 - val_accuracy: 0.9208\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.9193 - val_loss: 0.2846 - val_accuracy: 0.9208\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2896 - accuracy: 0.9192 - val_loss: 0.2844 - val_accuracy: 0.9207\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.9192 - val_loss: 0.2843 - val_accuracy: 0.9204\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.9196 - val_loss: 0.2840 - val_accuracy: 0.9202\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.9194 - val_loss: 0.2839 - val_accuracy: 0.9208\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.9196 - val_loss: 0.2837 - val_accuracy: 0.9207\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.9194 - val_loss: 0.2836 - val_accuracy: 0.9208\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2882 - accuracy: 0.9195 - val_loss: 0.2834 - val_accuracy: 0.9212\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.9195 - val_loss: 0.2833 - val_accuracy: 0.9207\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.9198 - val_loss: 0.2831 - val_accuracy: 0.9211\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.9198 - val_loss: 0.2830 - val_accuracy: 0.9210\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.9196 - val_loss: 0.2829 - val_accuracy: 0.9211\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.9200 - val_loss: 0.2828 - val_accuracy: 0.9208\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.9200 - val_loss: 0.2825 - val_accuracy: 0.9207\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.9199 - val_loss: 0.2825 - val_accuracy: 0.9204\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2864 - accuracy: 0.9199 - val_loss: 0.2823 - val_accuracy: 0.9214\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2862 - accuracy: 0.9200 - val_loss: 0.2822 - val_accuracy: 0.9208\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2860 - accuracy: 0.9199 - val_loss: 0.2820 - val_accuracy: 0.9208\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.9200 - val_loss: 0.2818 - val_accuracy: 0.9212\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2856 - accuracy: 0.9204 - val_loss: 0.2819 - val_accuracy: 0.9208\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2855 - accuracy: 0.9205 - val_loss: 0.2817 - val_accuracy: 0.9209\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.9201 - val_loss: 0.2815 - val_accuracy: 0.9216\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.9206 - val_loss: 0.2813 - val_accuracy: 0.9215\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.9203 - val_loss: 0.2813 - val_accuracy: 0.9212\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.9204 - val_loss: 0.2811 - val_accuracy: 0.9211\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.9206 - val_loss: 0.2811 - val_accuracy: 0.9208\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.9206 - val_loss: 0.2809 - val_accuracy: 0.9211\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2841 - accuracy: 0.9205 - val_loss: 0.2807 - val_accuracy: 0.9209\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.9207 - val_loss: 0.2808 - val_accuracy: 0.9207\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.9208 - val_loss: 0.2805 - val_accuracy: 0.9211\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2836 - accuracy: 0.9208 - val_loss: 0.2804 - val_accuracy: 0.9214\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.9209 - val_loss: 0.2803 - val_accuracy: 0.9209\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.9209 - val_loss: 0.2802 - val_accuracy: 0.9215\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2830 - accuracy: 0.9210 - val_loss: 0.2801 - val_accuracy: 0.9209\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2829 - accuracy: 0.9208 - val_loss: 0.2800 - val_accuracy: 0.9212\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.9210 - val_loss: 0.2799 - val_accuracy: 0.9212\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.9212 - val_loss: 0.2797 - val_accuracy: 0.9214\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.9213 - val_loss: 0.2796 - val_accuracy: 0.9209\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2822 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9217\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.9211 - val_loss: 0.2795 - val_accuracy: 0.9210\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2819 - accuracy: 0.9211 - val_loss: 0.2793 - val_accuracy: 0.9211\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.9213 - val_loss: 0.2792 - val_accuracy: 0.9214\n",
      "Epoch 163/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.9215 - val_loss: 0.2791 - val_accuracy: 0.9210\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.9215 - val_loss: 0.2790 - val_accuracy: 0.9215\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.9213 - val_loss: 0.2789 - val_accuracy: 0.9219\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.9218 - val_loss: 0.2788 - val_accuracy: 0.9218\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.9217 - val_loss: 0.2787 - val_accuracy: 0.9212\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.9213 - val_loss: 0.2787 - val_accuracy: 0.9225\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2806 - accuracy: 0.9217 - val_loss: 0.2785 - val_accuracy: 0.9222\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.9217 - val_loss: 0.2784 - val_accuracy: 0.9219\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9213\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.9215 - val_loss: 0.2783 - val_accuracy: 0.9218\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.9217 - val_loss: 0.2781 - val_accuracy: 0.9218\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.9217 - val_loss: 0.2781 - val_accuracy: 0.9217\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.9216 - val_loss: 0.2780 - val_accuracy: 0.9217\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.9217 - val_loss: 0.2779 - val_accuracy: 0.9222\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.9219 - val_loss: 0.2778 - val_accuracy: 0.9222\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.9218 - val_loss: 0.2777 - val_accuracy: 0.9225\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.9219 - val_loss: 0.2777 - val_accuracy: 0.9227\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.9218 - val_loss: 0.2776 - val_accuracy: 0.9218\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.9222 - val_loss: 0.2774 - val_accuracy: 0.9222\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.9220 - val_loss: 0.2773 - val_accuracy: 0.9221\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.9220 - val_loss: 0.2773 - val_accuracy: 0.9219\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.9221 - val_loss: 0.2772 - val_accuracy: 0.9222\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2783 - accuracy: 0.9221 - val_loss: 0.2771 - val_accuracy: 0.9222\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2782 - accuracy: 0.9222 - val_loss: 0.2770 - val_accuracy: 0.9222\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.9220 - val_loss: 0.2770 - val_accuracy: 0.9222\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.9221 - val_loss: 0.2769 - val_accuracy: 0.9222\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2778 - accuracy: 0.9221 - val_loss: 0.2768 - val_accuracy: 0.9222\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.9221 - val_loss: 0.2767 - val_accuracy: 0.9222\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.9222 - val_loss: 0.2767 - val_accuracy: 0.9222\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2774 - accuracy: 0.9223 - val_loss: 0.2765 - val_accuracy: 0.9223\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.9222 - val_loss: 0.2765 - val_accuracy: 0.9223\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2771 - accuracy: 0.9225 - val_loss: 0.2764 - val_accuracy: 0.9227\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.9224 - val_loss: 0.2764 - val_accuracy: 0.9234\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2769 - accuracy: 0.9227 - val_loss: 0.2763 - val_accuracy: 0.9227\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 0s 970us/step - loss: 0.2767 - accuracy: 0.9224 - val_loss: 0.2763 - val_accuracy: 0.9225\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 0s 952us/step - loss: 0.2766 - accuracy: 0.9226 - val_loss: 0.2762 - val_accuracy: 0.9223\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.9224 - val_loss: 0.2761 - val_accuracy: 0.9228\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1afa8b46c10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "               batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "               verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hay que tomar en cuenta que hemos reservado parte del conjunto de entrenamiento para la validación. La idea clave es que reservamos una parte de los datos de entrenamiento para medir el rendimiento en la validación durante el entrenamiento. Esta es una buena práctica a seguir para cualquier tarea de aprendizaje automático. \n",
    "\n",
    "Una vez que se entrena el modelo, podemos evaluarlo en el conjunto de prueba que contiene nuevos ejemplos nunca vistos por el modelo durante la fase de entrenamiento.\n",
    "\n",
    "En TensorFlow 2.0, podemos usar el método de evaluación (X_test, Y_test) para calcular test_loss y test_acc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 553us/step - loss: 0.2775 - accuracy: 0.9223\n",
      "Test accuracy: 0.9222999811172485\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejorando la red simple en TensorFlow 2.0 con capas ocultas\n",
    "\n",
    "Es un buen punto de partida, pero podemos mejorarlo. Veamos cómo.\n",
    "\n",
    "Una mejora inicial es agregar capas adicionales a nuestra red porque estas neuronas adicionales podrían ayudarla intuitivamente a aprender patrones más complejos en los datos de entrenamiento. En otras palabras, las capas adicionales agregan más parámetros, lo que potencialmente permite que un modelo memorice patrones más complejos. Entonces, después de la capa de entrada, tenemos una primera capa densa con N_HIDDEN neuronas y una función de activación \"ReLU\". Esta capa adicional se considera oculta porque no está conectada directamente ni con la entrada ni con la salida. Después de la primera capa oculta, tenemos una segunda capa oculta nuevamente con N_HIDDEN neuronas seguida de una capa de salida con 10 neuronas, cada una de las cuales se disparará cuando se reconozca el dígito relativo. El siguiente código define esta nueva red:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.4136 - accuracy: 0.6522 - val_loss: 0.7032 - val_accuracy: 0.8457\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5685 - accuracy: 0.8580 - val_loss: 0.4374 - val_accuracy: 0.8838\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.4250 - accuracy: 0.8846 - val_loss: 0.3668 - val_accuracy: 0.8967\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3698 - accuracy: 0.8976 - val_loss: 0.3303 - val_accuracy: 0.9050\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3379 - accuracy: 0.9047 - val_loss: 0.3069 - val_accuracy: 0.9119\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.3154 - accuracy: 0.9112 - val_loss: 0.2892 - val_accuracy: 0.9175\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2979 - accuracy: 0.9159 - val_loss: 0.2765 - val_accuracy: 0.9210\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2836 - accuracy: 0.9197 - val_loss: 0.2642 - val_accuracy: 0.9245\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2715 - accuracy: 0.9235 - val_loss: 0.2546 - val_accuracy: 0.9274\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2606 - accuracy: 0.9261 - val_loss: 0.2460 - val_accuracy: 0.9306\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2509 - accuracy: 0.9290 - val_loss: 0.2374 - val_accuracy: 0.9337\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2426 - accuracy: 0.9317 - val_loss: 0.2306 - val_accuracy: 0.9356\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2342 - accuracy: 0.9338 - val_loss: 0.2271 - val_accuracy: 0.9360\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2268 - accuracy: 0.9358 - val_loss: 0.2189 - val_accuracy: 0.9385\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2200 - accuracy: 0.9370 - val_loss: 0.2127 - val_accuracy: 0.9417\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2136 - accuracy: 0.9392 - val_loss: 0.2088 - val_accuracy: 0.9419\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2074 - accuracy: 0.9409 - val_loss: 0.2043 - val_accuracy: 0.9431\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.2017 - accuracy: 0.9428 - val_loss: 0.1979 - val_accuracy: 0.9460\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1964 - accuracy: 0.9444 - val_loss: 0.1946 - val_accuracy: 0.9466\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1910 - accuracy: 0.9459 - val_loss: 0.1892 - val_accuracy: 0.9490\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1861 - accuracy: 0.9472 - val_loss: 0.1860 - val_accuracy: 0.9488\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1813 - accuracy: 0.9480 - val_loss: 0.1819 - val_accuracy: 0.9512\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.9497 - val_loss: 0.1784 - val_accuracy: 0.9510\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1726 - accuracy: 0.9509 - val_loss: 0.1760 - val_accuracy: 0.9513\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1685 - accuracy: 0.9514 - val_loss: 0.1719 - val_accuracy: 0.9517\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1646 - accuracy: 0.9530 - val_loss: 0.1683 - val_accuracy: 0.9532\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1608 - accuracy: 0.9538 - val_loss: 0.1668 - val_accuracy: 0.9533\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1572 - accuracy: 0.9554 - val_loss: 0.1637 - val_accuracy: 0.9540\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1535 - accuracy: 0.9561 - val_loss: 0.1608 - val_accuracy: 0.9549\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1501 - accuracy: 0.9570 - val_loss: 0.1583 - val_accuracy: 0.9548\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1471 - accuracy: 0.9579 - val_loss: 0.1558 - val_accuracy: 0.9556\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1438 - accuracy: 0.9590 - val_loss: 0.1530 - val_accuracy: 0.9563\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1408 - accuracy: 0.9596 - val_loss: 0.1514 - val_accuracy: 0.9563\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1379 - accuracy: 0.9610 - val_loss: 0.1496 - val_accuracy: 0.9569\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1350 - accuracy: 0.9617 - val_loss: 0.1473 - val_accuracy: 0.9572\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1323 - accuracy: 0.9620 - val_loss: 0.1454 - val_accuracy: 0.9578\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1298 - accuracy: 0.9629 - val_loss: 0.1435 - val_accuracy: 0.9583\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1272 - accuracy: 0.9636 - val_loss: 0.1418 - val_accuracy: 0.9603\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1246 - accuracy: 0.9647 - val_loss: 0.1398 - val_accuracy: 0.9608\n",
      "Epoch 40/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9651 - val_loss: 0.1383 - val_accuracy: 0.9603\n",
      "Epoch 41/50\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9663 - val_loss: 0.1361 - val_accuracy: 0.9619\n",
      "Epoch 42/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9660 - val_loss: 0.1358 - val_accuracy: 0.9611\n",
      "Epoch 43/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.9676 - val_loss: 0.1328 - val_accuracy: 0.9626\n",
      "Epoch 44/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.9680 - val_loss: 0.1328 - val_accuracy: 0.9623\n",
      "Epoch 45/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1114 - accuracy: 0.9688 - val_loss: 0.1303 - val_accuracy: 0.9632\n",
      "Epoch 46/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1093 - accuracy: 0.9694 - val_loss: 0.1294 - val_accuracy: 0.9636\n",
      "Epoch 47/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1073 - accuracy: 0.9696 - val_loss: 0.1278 - val_accuracy: 0.9649\n",
      "Epoch 48/50\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1056 - accuracy: 0.9697 - val_loss: 0.1259 - val_accuracy: 0.9649\n",
      "Epoch 49/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1035 - accuracy: 0.9707 - val_loss: 0.1247 - val_accuracy: 0.9648\n",
      "Epoch 50/50\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 0.1017 - accuracy: 0.9713 - val_loss: 0.1246 - val_accuracy: 0.9647\n",
      "313/313 [==============================] - 0s 639us/step - loss: 0.1192 - accuracy: 0.9652\n",
      "Test accuracy: 0.9652000069618225\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Network and training.\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs to be within in [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# Labels have one-hot representation.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "# Build the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "          input_shape=(RESHAPED,),\n",
    "          name='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "          name='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "          name='dense_layer_3', activation='softmax'))\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejorar aún más la red simple en TensorFlow con Dropout\n",
    "\n",
    "Una segunda mejora es muy simple. Decidimos descartar aleatoriamente, con la probabilidad DROPOUT, algunos de los valores propagados dentro de nuestra densa red interna de capas ocultas durante el entrenamiento. En el aprendizaje automático, esta es una forma bien conocida de regularización. Sorprendentemente, esta idea de descartar aleatoriamente algunos valores puede mejorar nuestro rendimiento. La idea detrás de esta mejora es que la caída aleatoria obliga a la red a aprender patrones redundantes que son útiles para una mejor generalización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_layer (Dense)          (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_2 (Dense)        (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer_3 (Dense)        (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 1.6820 - accuracy: 0.4724 - val_loss: 0.8607 - val_accuracy: 0.8258\n",
      "Epoch 2/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.8910 - accuracy: 0.7281 - val_loss: 0.5149 - val_accuracy: 0.8778\n",
      "Epoch 3/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.6795 - accuracy: 0.7910 - val_loss: 0.4152 - val_accuracy: 0.8904\n",
      "Epoch 4/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5811 - accuracy: 0.8248 - val_loss: 0.3662 - val_accuracy: 0.9001\n",
      "Epoch 5/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.5226 - accuracy: 0.8437 - val_loss: 0.3318 - val_accuracy: 0.9068\n",
      "Epoch 6/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4804 - accuracy: 0.8572 - val_loss: 0.3099 - val_accuracy: 0.9123\n",
      "Epoch 7/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4477 - accuracy: 0.8689 - val_loss: 0.2932 - val_accuracy: 0.9167\n",
      "Epoch 8/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4254 - accuracy: 0.8760 - val_loss: 0.2776 - val_accuracy: 0.9198\n",
      "Epoch 9/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.4057 - accuracy: 0.8796 - val_loss: 0.2657 - val_accuracy: 0.9233\n",
      "Epoch 10/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3831 - accuracy: 0.8874 - val_loss: 0.2549 - val_accuracy: 0.9267\n",
      "Epoch 11/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3700 - accuracy: 0.8910 - val_loss: 0.2475 - val_accuracy: 0.9281\n",
      "Epoch 12/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3562 - accuracy: 0.8962 - val_loss: 0.2370 - val_accuracy: 0.9317\n",
      "Epoch 13/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3446 - accuracy: 0.8993 - val_loss: 0.2296 - val_accuracy: 0.9344\n",
      "Epoch 14/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3349 - accuracy: 0.9030 - val_loss: 0.2226 - val_accuracy: 0.9361\n",
      "Epoch 15/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3259 - accuracy: 0.9059 - val_loss: 0.2160 - val_accuracy: 0.9386\n",
      "Epoch 16/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3167 - accuracy: 0.9075 - val_loss: 0.2111 - val_accuracy: 0.9398\n",
      "Epoch 17/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.3045 - accuracy: 0.9101 - val_loss: 0.2046 - val_accuracy: 0.9421\n",
      "Epoch 18/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2975 - accuracy: 0.9134 - val_loss: 0.1993 - val_accuracy: 0.9433\n",
      "Epoch 19/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2902 - accuracy: 0.9149 - val_loss: 0.1950 - val_accuracy: 0.9448\n",
      "Epoch 20/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2876 - accuracy: 0.9166 - val_loss: 0.1909 - val_accuracy: 0.9466\n",
      "Epoch 21/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2776 - accuracy: 0.9193 - val_loss: 0.1857 - val_accuracy: 0.9483\n",
      "Epoch 22/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2720 - accuracy: 0.9208 - val_loss: 0.1822 - val_accuracy: 0.9494\n",
      "Epoch 23/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2635 - accuracy: 0.9223 - val_loss: 0.1788 - val_accuracy: 0.9507\n",
      "Epoch 24/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2622 - accuracy: 0.9231 - val_loss: 0.1752 - val_accuracy: 0.9513\n",
      "Epoch 25/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2542 - accuracy: 0.9261 - val_loss: 0.1711 - val_accuracy: 0.9526\n",
      "Epoch 26/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2513 - accuracy: 0.9273 - val_loss: 0.1689 - val_accuracy: 0.9528\n",
      "Epoch 27/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2455 - accuracy: 0.9289 - val_loss: 0.1658 - val_accuracy: 0.9534\n",
      "Epoch 28/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2382 - accuracy: 0.9303 - val_loss: 0.1628 - val_accuracy: 0.9548\n",
      "Epoch 29/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2383 - accuracy: 0.9301 - val_loss: 0.1601 - val_accuracy: 0.9551\n",
      "Epoch 30/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2376 - accuracy: 0.9300 - val_loss: 0.1579 - val_accuracy: 0.9556\n",
      "Epoch 31/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2308 - accuracy: 0.9329 - val_loss: 0.1553 - val_accuracy: 0.9565\n",
      "Epoch 32/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2270 - accuracy: 0.9333 - val_loss: 0.1526 - val_accuracy: 0.9572\n",
      "Epoch 33/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2229 - accuracy: 0.9346 - val_loss: 0.1510 - val_accuracy: 0.9579\n",
      "Epoch 34/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2208 - accuracy: 0.9360 - val_loss: 0.1492 - val_accuracy: 0.9585\n",
      "Epoch 35/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2160 - accuracy: 0.9373 - val_loss: 0.1473 - val_accuracy: 0.9592\n",
      "Epoch 36/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2119 - accuracy: 0.9382 - val_loss: 0.1454 - val_accuracy: 0.9590\n",
      "Epoch 37/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9383 - val_loss: 0.1434 - val_accuracy: 0.9595\n",
      "Epoch 38/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2082 - accuracy: 0.9403 - val_loss: 0.1418 - val_accuracy: 0.9599\n",
      "Epoch 39/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9402 - val_loss: 0.1401 - val_accuracy: 0.9603\n",
      "Epoch 40/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9410 - val_loss: 0.1384 - val_accuracy: 0.9603\n",
      "Epoch 41/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9423 - val_loss: 0.1377 - val_accuracy: 0.9614\n",
      "Epoch 42/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9418 - val_loss: 0.1357 - val_accuracy: 0.9621\n",
      "Epoch 43/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9437 - val_loss: 0.1344 - val_accuracy: 0.9618\n",
      "Epoch 44/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9436 - val_loss: 0.1329 - val_accuracy: 0.9621\n",
      "Epoch 45/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9442 - val_loss: 0.1315 - val_accuracy: 0.9629\n",
      "Epoch 46/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9435 - val_loss: 0.1305 - val_accuracy: 0.9630\n",
      "Epoch 47/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9438 - val_loss: 0.1289 - val_accuracy: 0.9635\n",
      "Epoch 48/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9471 - val_loss: 0.1285 - val_accuracy: 0.9640\n",
      "Epoch 49/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9474 - val_loss: 0.1269 - val_accuracy: 0.9639\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9473 - val_loss: 0.1256 - val_accuracy: 0.9644\n",
      "Epoch 51/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9482 - val_loss: 0.1252 - val_accuracy: 0.9643\n",
      "Epoch 52/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9492 - val_loss: 0.1240 - val_accuracy: 0.9648\n",
      "Epoch 53/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1725 - accuracy: 0.9498 - val_loss: 0.1228 - val_accuracy: 0.9653\n",
      "Epoch 54/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9493 - val_loss: 0.1216 - val_accuracy: 0.9657\n",
      "Epoch 55/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1711 - accuracy: 0.9496 - val_loss: 0.1213 - val_accuracy: 0.9656\n",
      "Epoch 56/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.9498 - val_loss: 0.1194 - val_accuracy: 0.9667\n",
      "Epoch 57/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1656 - accuracy: 0.9512 - val_loss: 0.1193 - val_accuracy: 0.9664\n",
      "Epoch 58/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1629 - accuracy: 0.9520 - val_loss: 0.1176 - val_accuracy: 0.9673\n",
      "Epoch 59/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1633 - accuracy: 0.9516 - val_loss: 0.1174 - val_accuracy: 0.9670\n",
      "Epoch 60/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1625 - accuracy: 0.9516 - val_loss: 0.1164 - val_accuracy: 0.9673\n",
      "Epoch 61/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1617 - accuracy: 0.9527 - val_loss: 0.1164 - val_accuracy: 0.9669\n",
      "Epoch 62/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1584 - accuracy: 0.9532 - val_loss: 0.1157 - val_accuracy: 0.9669\n",
      "Epoch 63/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1590 - accuracy: 0.9528 - val_loss: 0.1145 - val_accuracy: 0.9678\n",
      "Epoch 64/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1544 - accuracy: 0.9542 - val_loss: 0.1132 - val_accuracy: 0.9679\n",
      "Epoch 65/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1534 - accuracy: 0.9550 - val_loss: 0.1125 - val_accuracy: 0.9684\n",
      "Epoch 66/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1516 - accuracy: 0.9558 - val_loss: 0.1121 - val_accuracy: 0.9683\n",
      "Epoch 67/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1520 - accuracy: 0.9553 - val_loss: 0.1117 - val_accuracy: 0.9682\n",
      "Epoch 68/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1497 - accuracy: 0.9564 - val_loss: 0.1107 - val_accuracy: 0.9683\n",
      "Epoch 69/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1527 - accuracy: 0.9551 - val_loss: 0.1107 - val_accuracy: 0.9687\n",
      "Epoch 70/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1477 - accuracy: 0.9560 - val_loss: 0.1093 - val_accuracy: 0.9693\n",
      "Epoch 71/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1489 - accuracy: 0.9561 - val_loss: 0.1093 - val_accuracy: 0.9685\n",
      "Epoch 72/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1427 - accuracy: 0.9579 - val_loss: 0.1087 - val_accuracy: 0.9689\n",
      "Epoch 73/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1459 - accuracy: 0.9568 - val_loss: 0.1075 - val_accuracy: 0.9701\n",
      "Epoch 74/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1443 - accuracy: 0.9574 - val_loss: 0.1081 - val_accuracy: 0.9690\n",
      "Epoch 75/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1417 - accuracy: 0.9578 - val_loss: 0.1067 - val_accuracy: 0.9700\n",
      "Epoch 76/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1407 - accuracy: 0.9578 - val_loss: 0.1065 - val_accuracy: 0.9695\n",
      "Epoch 77/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1389 - accuracy: 0.9595 - val_loss: 0.1057 - val_accuracy: 0.9697\n",
      "Epoch 78/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1380 - accuracy: 0.9589 - val_loss: 0.1048 - val_accuracy: 0.9701\n",
      "Epoch 79/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1380 - accuracy: 0.9590 - val_loss: 0.1043 - val_accuracy: 0.9704\n",
      "Epoch 80/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1377 - accuracy: 0.9589 - val_loss: 0.1041 - val_accuracy: 0.9700\n",
      "Epoch 81/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1338 - accuracy: 0.9606 - val_loss: 0.1038 - val_accuracy: 0.9699\n",
      "Epoch 82/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1320 - accuracy: 0.9605 - val_loss: 0.1036 - val_accuracy: 0.9702\n",
      "Epoch 83/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1343 - accuracy: 0.9600 - val_loss: 0.1027 - val_accuracy: 0.9708\n",
      "Epoch 84/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9610 - val_loss: 0.1024 - val_accuracy: 0.9705\n",
      "Epoch 85/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1322 - accuracy: 0.9605 - val_loss: 0.1022 - val_accuracy: 0.9707\n",
      "Epoch 86/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1324 - accuracy: 0.9611 - val_loss: 0.1016 - val_accuracy: 0.9709\n",
      "Epoch 87/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1275 - accuracy: 0.9615 - val_loss: 0.1021 - val_accuracy: 0.9707\n",
      "Epoch 88/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1296 - accuracy: 0.9615 - val_loss: 0.1010 - val_accuracy: 0.9709\n",
      "Epoch 89/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1285 - accuracy: 0.9614 - val_loss: 0.1000 - val_accuracy: 0.9715\n",
      "Epoch 90/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1263 - accuracy: 0.9625 - val_loss: 0.0997 - val_accuracy: 0.9719\n",
      "Epoch 91/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1269 - accuracy: 0.9614 - val_loss: 0.0996 - val_accuracy: 0.9712\n",
      "Epoch 92/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9626 - val_loss: 0.0986 - val_accuracy: 0.9716\n",
      "Epoch 93/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9629 - val_loss: 0.0980 - val_accuracy: 0.9718\n",
      "Epoch 94/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1248 - accuracy: 0.9630 - val_loss: 0.0985 - val_accuracy: 0.9715\n",
      "Epoch 95/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1218 - accuracy: 0.9632 - val_loss: 0.0984 - val_accuracy: 0.9712\n",
      "Epoch 96/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1197 - accuracy: 0.9633 - val_loss: 0.0979 - val_accuracy: 0.9723\n",
      "Epoch 97/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1193 - accuracy: 0.9643 - val_loss: 0.0982 - val_accuracy: 0.9722\n",
      "Epoch 98/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1203 - accuracy: 0.9646 - val_loss: 0.0983 - val_accuracy: 0.9722\n",
      "Epoch 99/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1201 - accuracy: 0.9642 - val_loss: 0.0976 - val_accuracy: 0.9722\n",
      "Epoch 100/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1198 - accuracy: 0.9650 - val_loss: 0.0971 - val_accuracy: 0.9718\n",
      "Epoch 101/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1171 - accuracy: 0.9651 - val_loss: 0.0968 - val_accuracy: 0.9723\n",
      "Epoch 102/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.9653 - val_loss: 0.0964 - val_accuracy: 0.9726\n",
      "Epoch 103/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1161 - accuracy: 0.9661 - val_loss: 0.0960 - val_accuracy: 0.9722\n",
      "Epoch 104/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1147 - accuracy: 0.9655 - val_loss: 0.0958 - val_accuracy: 0.9723\n",
      "Epoch 105/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1145 - accuracy: 0.9665 - val_loss: 0.0961 - val_accuracy: 0.9724\n",
      "Epoch 106/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1114 - accuracy: 0.9672 - val_loss: 0.0960 - val_accuracy: 0.9727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1157 - accuracy: 0.9652 - val_loss: 0.0948 - val_accuracy: 0.9725\n",
      "Epoch 108/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1117 - accuracy: 0.9667 - val_loss: 0.0949 - val_accuracy: 0.9731\n",
      "Epoch 109/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1120 - accuracy: 0.9670 - val_loss: 0.0946 - val_accuracy: 0.9726\n",
      "Epoch 110/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1112 - accuracy: 0.9669 - val_loss: 0.0940 - val_accuracy: 0.9726\n",
      "Epoch 111/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1132 - accuracy: 0.9666 - val_loss: 0.0939 - val_accuracy: 0.9728\n",
      "Epoch 112/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1101 - accuracy: 0.9676 - val_loss: 0.0933 - val_accuracy: 0.9731\n",
      "Epoch 113/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1108 - accuracy: 0.9668 - val_loss: 0.0933 - val_accuracy: 0.9732\n",
      "Epoch 114/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1077 - accuracy: 0.9677 - val_loss: 0.0937 - val_accuracy: 0.9733\n",
      "Epoch 115/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1096 - accuracy: 0.9663 - val_loss: 0.0929 - val_accuracy: 0.9731\n",
      "Epoch 116/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1067 - accuracy: 0.9684 - val_loss: 0.0927 - val_accuracy: 0.9734\n",
      "Epoch 117/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.9676 - val_loss: 0.0924 - val_accuracy: 0.9730\n",
      "Epoch 118/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1042 - accuracy: 0.9684 - val_loss: 0.0926 - val_accuracy: 0.9730\n",
      "Epoch 119/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1068 - accuracy: 0.9676 - val_loss: 0.0916 - val_accuracy: 0.9731\n",
      "Epoch 120/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.9669 - val_loss: 0.0914 - val_accuracy: 0.9736\n",
      "Epoch 121/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1061 - accuracy: 0.9673 - val_loss: 0.0915 - val_accuracy: 0.9738\n",
      "Epoch 122/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1038 - accuracy: 0.9692 - val_loss: 0.0914 - val_accuracy: 0.9734\n",
      "Epoch 123/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1031 - accuracy: 0.9700 - val_loss: 0.0913 - val_accuracy: 0.9737\n",
      "Epoch 124/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1033 - accuracy: 0.9683 - val_loss: 0.0910 - val_accuracy: 0.9732\n",
      "Epoch 125/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1011 - accuracy: 0.9711 - val_loss: 0.0903 - val_accuracy: 0.9737\n",
      "Epoch 126/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1025 - accuracy: 0.9697 - val_loss: 0.0907 - val_accuracy: 0.9742\n",
      "Epoch 127/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1021 - accuracy: 0.9689 - val_loss: 0.0893 - val_accuracy: 0.9739\n",
      "Epoch 128/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1016 - accuracy: 0.9686 - val_loss: 0.0898 - val_accuracy: 0.9745\n",
      "Epoch 129/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.1006 - accuracy: 0.9693 - val_loss: 0.0894 - val_accuracy: 0.9743\n",
      "Epoch 130/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0997 - accuracy: 0.9693 - val_loss: 0.0903 - val_accuracy: 0.9742\n",
      "Epoch 131/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0990 - accuracy: 0.9697 - val_loss: 0.0893 - val_accuracy: 0.9740\n",
      "Epoch 132/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0965 - accuracy: 0.9703 - val_loss: 0.0892 - val_accuracy: 0.9750\n",
      "Epoch 133/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0981 - accuracy: 0.9704 - val_loss: 0.0889 - val_accuracy: 0.9744\n",
      "Epoch 134/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0993 - accuracy: 0.9696 - val_loss: 0.0891 - val_accuracy: 0.9744\n",
      "Epoch 135/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0973 - accuracy: 0.9711 - val_loss: 0.0888 - val_accuracy: 0.9740\n",
      "Epoch 136/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0977 - accuracy: 0.9707 - val_loss: 0.0895 - val_accuracy: 0.9738\n",
      "Epoch 137/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0954 - accuracy: 0.9710 - val_loss: 0.0889 - val_accuracy: 0.9744\n",
      "Epoch 138/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0928 - accuracy: 0.9722 - val_loss: 0.0886 - val_accuracy: 0.9743\n",
      "Epoch 139/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.9716 - val_loss: 0.0881 - val_accuracy: 0.9750\n",
      "Epoch 140/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0938 - accuracy: 0.9712 - val_loss: 0.0877 - val_accuracy: 0.9747\n",
      "Epoch 141/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0967 - accuracy: 0.9710 - val_loss: 0.0877 - val_accuracy: 0.9749\n",
      "Epoch 142/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0952 - accuracy: 0.9709 - val_loss: 0.0877 - val_accuracy: 0.9752\n",
      "Epoch 143/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0941 - accuracy: 0.9712 - val_loss: 0.0875 - val_accuracy: 0.9743\n",
      "Epoch 144/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0935 - accuracy: 0.9712 - val_loss: 0.0873 - val_accuracy: 0.9744\n",
      "Epoch 145/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0914 - accuracy: 0.9727 - val_loss: 0.0870 - val_accuracy: 0.9750\n",
      "Epoch 146/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0922 - accuracy: 0.9728 - val_loss: 0.0870 - val_accuracy: 0.9746\n",
      "Epoch 147/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0893 - accuracy: 0.9728 - val_loss: 0.0874 - val_accuracy: 0.9751\n",
      "Epoch 148/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0937 - accuracy: 0.9718 - val_loss: 0.0863 - val_accuracy: 0.9753\n",
      "Epoch 149/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0902 - accuracy: 0.9727 - val_loss: 0.0875 - val_accuracy: 0.9748\n",
      "Epoch 150/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9724 - val_loss: 0.0872 - val_accuracy: 0.9744\n",
      "Epoch 151/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0899 - accuracy: 0.9726 - val_loss: 0.0865 - val_accuracy: 0.9752\n",
      "Epoch 152/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0911 - accuracy: 0.9719 - val_loss: 0.0865 - val_accuracy: 0.9747\n",
      "Epoch 153/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9724 - val_loss: 0.0867 - val_accuracy: 0.9753\n",
      "Epoch 154/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0885 - accuracy: 0.9730 - val_loss: 0.0861 - val_accuracy: 0.9751\n",
      "Epoch 155/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0875 - accuracy: 0.9731 - val_loss: 0.0862 - val_accuracy: 0.9748\n",
      "Epoch 156/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0859 - accuracy: 0.9740 - val_loss: 0.0859 - val_accuracy: 0.9752\n",
      "Epoch 157/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0860 - accuracy: 0.9739 - val_loss: 0.0862 - val_accuracy: 0.9751\n",
      "Epoch 158/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0864 - accuracy: 0.9734 - val_loss: 0.0853 - val_accuracy: 0.9755\n",
      "Epoch 159/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.9739 - val_loss: 0.0847 - val_accuracy: 0.9751\n",
      "Epoch 160/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.9737 - val_loss: 0.0849 - val_accuracy: 0.9758\n",
      "Epoch 161/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0845 - accuracy: 0.9738 - val_loss: 0.0852 - val_accuracy: 0.9753\n",
      "Epoch 162/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0849 - accuracy: 0.9743 - val_loss: 0.0850 - val_accuracy: 0.9755\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0872 - accuracy: 0.9732 - val_loss: 0.0847 - val_accuracy: 0.9757\n",
      "Epoch 164/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0851 - accuracy: 0.9748 - val_loss: 0.0844 - val_accuracy: 0.9755\n",
      "Epoch 165/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9739 - val_loss: 0.0848 - val_accuracy: 0.9753\n",
      "Epoch 166/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9743 - val_loss: 0.0848 - val_accuracy: 0.9750\n",
      "Epoch 167/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0855 - accuracy: 0.9729 - val_loss: 0.0842 - val_accuracy: 0.9758\n",
      "Epoch 168/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0846 - accuracy: 0.9747 - val_loss: 0.0848 - val_accuracy: 0.9752\n",
      "Epoch 169/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0843 - accuracy: 0.9746 - val_loss: 0.0847 - val_accuracy: 0.9753\n",
      "Epoch 170/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9752 - val_loss: 0.0838 - val_accuracy: 0.9751\n",
      "Epoch 171/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9737 - val_loss: 0.0837 - val_accuracy: 0.9753\n",
      "Epoch 172/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9751 - val_loss: 0.0843 - val_accuracy: 0.9749\n",
      "Epoch 173/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0825 - accuracy: 0.9752 - val_loss: 0.0840 - val_accuracy: 0.9754\n",
      "Epoch 174/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9752 - val_loss: 0.0838 - val_accuracy: 0.9754\n",
      "Epoch 175/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0801 - accuracy: 0.9756 - val_loss: 0.0833 - val_accuracy: 0.9755\n",
      "Epoch 176/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0806 - accuracy: 0.9756 - val_loss: 0.0839 - val_accuracy: 0.9753\n",
      "Epoch 177/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0794 - accuracy: 0.9753 - val_loss: 0.0835 - val_accuracy: 0.9752\n",
      "Epoch 178/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0791 - accuracy: 0.9761 - val_loss: 0.0832 - val_accuracy: 0.9754\n",
      "Epoch 179/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0807 - accuracy: 0.9757 - val_loss: 0.0835 - val_accuracy: 0.9753\n",
      "Epoch 180/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9755 - val_loss: 0.0838 - val_accuracy: 0.9757\n",
      "Epoch 181/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9762 - val_loss: 0.0831 - val_accuracy: 0.9758\n",
      "Epoch 182/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0787 - accuracy: 0.9762 - val_loss: 0.0832 - val_accuracy: 0.9757\n",
      "Epoch 183/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9759 - val_loss: 0.0829 - val_accuracy: 0.9759\n",
      "Epoch 184/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0785 - accuracy: 0.9761 - val_loss: 0.0828 - val_accuracy: 0.9758\n",
      "Epoch 185/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0774 - accuracy: 0.9768 - val_loss: 0.0828 - val_accuracy: 0.9758\n",
      "Epoch 186/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0783 - accuracy: 0.9756 - val_loss: 0.0830 - val_accuracy: 0.9757\n",
      "Epoch 187/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0752 - accuracy: 0.9771 - val_loss: 0.0830 - val_accuracy: 0.9758\n",
      "Epoch 188/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.9762 - val_loss: 0.0831 - val_accuracy: 0.9756\n",
      "Epoch 189/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0758 - accuracy: 0.9771 - val_loss: 0.0826 - val_accuracy: 0.9762\n",
      "Epoch 190/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9759 - val_loss: 0.0827 - val_accuracy: 0.9763\n",
      "Epoch 191/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9775 - val_loss: 0.0825 - val_accuracy: 0.9759\n",
      "Epoch 192/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.9766 - val_loss: 0.0827 - val_accuracy: 0.9761\n",
      "Epoch 193/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9760 - val_loss: 0.0826 - val_accuracy: 0.9757\n",
      "Epoch 194/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.0825 - val_accuracy: 0.9758\n",
      "Epoch 195/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9776 - val_loss: 0.0826 - val_accuracy: 0.9763\n",
      "Epoch 196/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0753 - accuracy: 0.9771 - val_loss: 0.0818 - val_accuracy: 0.9767\n",
      "Epoch 197/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9765 - val_loss: 0.0823 - val_accuracy: 0.9758\n",
      "Epoch 198/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9773 - val_loss: 0.0818 - val_accuracy: 0.9760\n",
      "Epoch 199/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9770 - val_loss: 0.0821 - val_accuracy: 0.9758\n",
      "Epoch 200/200\n",
      "375/375 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.9779 - val_loss: 0.0815 - val_accuracy: 0.9762\n",
      "313/313 [==============================] - 0s 700us/step - loss: 0.0721 - accuracy: 0.9770\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "# Network and training.\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "DROPOUT = 0.3\n",
    "# Loading MNIST dataset.\n",
    "# Labels have one-hot representation.\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize inputs within [0, 1].\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# One-hot representations for labels.\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n",
    "# Building the model.\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "          input_shape=(RESHAPED,),\n",
    "          name='dense_layer', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(N_HIDDEN,\n",
    "          name='dense_layer_2', activation='relu'))\n",
    "model.add(keras.layers.Dropout(DROPOUT))\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "          name='dense_layer_3', activation='softmax'))\n",
    "# Summary of the model.\n",
    "model.summary()\n",
    "# Compiling the model.\n",
    "model.compile(optimizer='SGD', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Training the model.\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "# Evaluating the model.\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
