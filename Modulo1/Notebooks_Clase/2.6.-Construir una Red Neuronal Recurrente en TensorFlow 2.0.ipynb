{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# <center> <font color= #000047> Módulo 1: Crear una Red Neuronal Recurrente en TensorFlow 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JL3SBH6PzDwV"
   },
   "source": [
    "## Paso 1: Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ynShOu8nNtFt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1557527181981,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "Kw7-sPdOzf5l",
    "outputId": "43c449a9-bafd-408f-b6e1-17612b5fd375"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEjlM2EazOf0"
   },
   "source": [
    "## Paso 2: Pre procesado de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wB0tNtXJzTfA"
   },
   "source": [
    "### Configurar parámetros del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jw6_KU24SrYK"
   },
   "outputs": [],
   "source": [
    "number_of_words = 20000\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePywR8A4zaxT"
   },
   "source": [
    "### Carga del dataset de IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kCTV_hjOKmE"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=number_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZKDNoTKzi5w"
   },
   "source": [
    "### Cortar secuencias de texto de la misma longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHcMNzv7Pd1s"
   },
   "outputs": [],
   "source": [
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fcxd--ESP3Rh"
   },
   "outputs": [],
   "source": [
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xDMP44Zz0dU"
   },
   "source": [
    "### Configurar parámetros de la capa de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 583,
     "status": "ok",
     "timestamp": 1557527197664,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "nGHQ2upgQIGj",
    "outputId": "cd40fe5e-bd42-442a-801a-55e79865ab8c"
   },
   "outputs": [],
   "source": [
    "vocab_size = number_of_words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMyk2JcPQcjF"
   },
   "outputs": [],
   "source": [
    "embed_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VG6LBKGnz7jT"
   },
   "source": [
    "## Paso 3: Construir la Red Neuronal Recurrente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUVnz-9K0DcW"
   },
   "source": [
    "### Definir el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2GHzwk6OMrV"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnXJZYR-0HXE"
   },
   "source": [
    "### Añadir la capa de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWqC0DXbO9FU"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, embed_size, input_shape=(X_train.shape[1],)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM-lpTZX1mEG"
   },
   "source": [
    "### Añadir la capa de LSTM\n",
    "\n",
    "- unidades: 128\n",
    "- función de activación: tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U numpy==1.18.5 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1557527201016,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "5W7IXqhjQpAl",
    "outputId": "ca4438c1-f0a3-4742-9cb6-803920134d18"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.LSTM(units=128, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9T9M5Ult10XM"
   },
   "source": [
    "### Añadir la capa totalmente conectada de salida\n",
    "\n",
    "- unidades: 1\n",
    "- función de activación: sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xe1nHzq7Q91-"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWcqM4Yr2ALS"
   },
   "source": [
    "### Compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-z9ACOXcRUUN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1557527203541,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "PiolKKO6RjVF",
    "outputId": "6b165bfd-2c4d-434a-8bc1-42cc6ba91720"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bPUvbfe2GJI"
   },
   "source": [
    "### Entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9460,
     "status": "ok",
     "timestamp": 1557527214350,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "9FqUTA1CRpQ8",
    "outputId": "0d496788-f042-4fb3-d7d9-18166d2cbc9d"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wMo2wYpbCgb"
   },
   "source": [
    "### Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3908,
     "status": "ok",
     "timestamp": 1557527267061,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "a8kD_6q-RySO",
    "outputId": "4f792319-1095-42b8-c35a-fb8f2f5093e6"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acurracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 457,
     "status": "ok",
     "timestamp": 1557527277712,
     "user": {
      "displayName": "TechGuy Luka",
      "photoUrl": "https://lh3.googleusercontent.com/-v_8ixmE_SJc/AAAAAAAAAAI/AAAAAAAAAEY/QGGmNZURhdc/s64/photo.jpg",
      "userId": "12375685325186592450"
     },
     "user_tz": -120
    },
    "id": "C0XnUtS-cEeI",
    "outputId": "0d99df5f-717e-4751-c4dc-48e77d765056"
   },
   "outputs": [],
   "source": [
    "print(\"Test accuracy: {}\".format(test_acurracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fN9QK49W3C29"
   },
   "source": [
    "### Otro Ejemplo: Predecir el precio de las acciones de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('Data/Google_Stock_Price_Train.csv')\n",
    "\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling should always be applied for \n",
    "# Deep Learning models\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the memory of RNN depends on the number of timesteps you select\n",
    "# if timesteps = x then the output depends on the previous x inputs\n",
    "\n",
    "# Create input set that consists of 60 dimensions\n",
    "# hence, the output of current day will be based on \n",
    "# the prices of previous 60 days\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i - 60: i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "    \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape : convert our 2D array to 3D\n",
    "(batch_size, timesteps) = X_train.shape\n",
    "# This is done because the RNN class in Keras expects a \n",
    "# 3D Tensor\n",
    "X_train = np.reshape(X_train, (batch_size, timesteps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Sequential model\n",
    "regressor = Sequential()\n",
    "\n",
    "# units is the output dimensionality\n",
    "# return sequences will return the sequence\n",
    "# which will be required to the next LSTM \n",
    "\n",
    "# input shape will need only the last 2 dimensions\n",
    "# of your input\n",
    "################# 1st layer #######################\n",
    "regressor.add(LSTM(units=50, return_sequences=True, \n",
    "                   input_shape=(timesteps, 1)))\n",
    "\n",
    "# add Dropout to do regulariztion\n",
    "# standard practise to use 20%\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "################# 2nd layer ######################\n",
    "# After the first time, it's not required to \n",
    "# specify the input_shape\n",
    "\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "################# 3rd layer ######################\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "              \n",
    "\n",
    "################# 4th layer ######################\n",
    "# the last layer needn't return the sequence, so\n",
    "# return_sequences will be False\n",
    "              \n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "################# 5th layer ######################\n",
    "# Last layer would be the fully connected layer,\n",
    "# or the Dense layer\n",
    "#\n",
    "# The last word will predict a single number\n",
    "# hence units=1\n",
    "\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "# The loss function for classification problem is \n",
    "# cross entropy, since this is a regression problem\n",
    "# the loss function will be mean squared error\n",
    "              \n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting the model for future use\n",
    "from tensorflow.keras.models import load_model\n",
    "# Save\n",
    "regressor.save('regressor.hd5')\n",
    "# Load\n",
    "# model = load_model('regressor.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "dataset_test = pd.read_csv('Data/Google_Stock_Price_Test.csv')\n",
    "\n",
    "real_stock_prices = dataset_test.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test dataset\n",
    "#\n",
    "# To predict the price for 3 Jan '17 (first financial\n",
    "# day) we will need the price for 60 days before that, \n",
    "# so we will need both, the train and the test set to\n",
    "# gather the required data\n",
    "\n",
    "# 1. Concatenate training and test set\n",
    "\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']),\n",
    "                         axis=0) # axis=0 is concatenate rows\n",
    "\n",
    "# 2. For each day from 1st Jan get 60 previous \n",
    "#    days' data\n",
    "\n",
    "first_day_index = len(dataset_total) - len(dataset_test)\n",
    "inputs = dataset_total[first_day_index - 60: ].values\n",
    "inputs = inputs.reshape(-1, 1)\n",
    "\n",
    "# 3. Scale the input\n",
    "\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset to have 60 dimensions\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i - 60: i, 0])\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Reshape : convert our 2D array to 3D\n",
    "(batch_size, timesteps) = X_test.shape\n",
    "# This is done because the RNN class in Keras expects a \n",
    "# 3D Tensor\n",
    "X_test = np.reshape(X_test, (batch_size, timesteps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "# inverse the scaling\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualing the results\n",
    "plt.plot(real_stock_prices, color='red', \n",
    "         label='Real Google Stock Prices')\n",
    "plt.plot(predicted_stock_price, color='green',\n",
    "         label='Predicted Google Stock Prices')\n",
    "plt.title('Google Stock Price Prediction using RNN')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text data tokens are words or characters and any network that can model the probability of the next token is called language model. A language model captures the statistical structure of the text. If we are training the neural network to predict the next character, it is called Character Level Model. Similarly, we can train the model to predict the next word, given a sequence of words called Word Level Models. We are implementing character level model.\n",
    "\n",
    "<img src=\"Figures/63.png\" alt=\"Grayscale Image\" width=\"600\">\n",
    "\n",
    "### Implementing in Tensorflow\n",
    "#### The Dataset\n",
    "We will use a dataset which contains the works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#Download the dataset\n",
    "path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "#Explore the data\n",
    "text = open(path, \"r\").read()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "Before training we need to map strings to numbers, extract partially overlapping sequences and pack them in a 3D numpy array of shape (sequences, maxlen, unique_characters). We one-hot encode the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(file_path):\n",
    "    text = open(file_path, 'rb').read().decode(encoding='utf-8')  # Read, then decode for py2 compat.\n",
    "    vocab = sorted(set(text))  # The unique characters in the file\n",
    "    # Creating a mapping from unique characters to indices and vice versa\n",
    "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "    idx2char = np.array(vocab)\n",
    "    text_as_int = np.array([char2idx[c] for c in text])\n",
    "    return text_as_int, vocab, char2idx, idx2char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text, target_text = chunk[:-1], chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text_as_int, seq_length=100, batch_size=64, buffer_size=10000):\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "    dataset = char_dataset.batch(seq_length + 1, drop_remainder=True).map(split_input_target)\n",
    "    dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Network\n",
    "The network is a single LSTM layer followed by a Dense classifier and softmax over all possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=64):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int, vocab, char2idx, idx2char = process_text(path)\n",
    "dataset = create_dataset(text_as_int)\n",
    "model = build_model(vocab_size=len(vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the model\n",
    "Use the categorical_crossentropy loss to train the model as the targets are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, None, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (64, None, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 1024)          8392704   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, None, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (64, None, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 13,731,137\n",
      "Trainable params: 13,727,041\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 1469s 9s/step - loss: 2.0597\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 1882s 11s/step - loss: 1.6173\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 2159s 13s/step - loss: 1.4908\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 2380s 14s/step - loss: 1.4230\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 2601s 15s/step - loss: 1.3759\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 3092s 18s/step - loss: 1.3362\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 3829s 22s/step - loss: 1.3013\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 20437s 119s/step - loss: 1.2682\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 2462s 14s/step - loss: 1.2341\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 2439s 14s/step - loss: 1.1982\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (1, None, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 1024)           8392704   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, None, 1024)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (1, None, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 13,731,137\n",
      "Trainable params: 13,727,041\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=10)\n",
    "model.save_weights(\"gen_text_weights.h5\", save_format='h5')\n",
    "# To keep this prediction step simple, use a batch size of 1\n",
    "model = build_model(vocab_size=len(vocab), batch_size=1)\n",
    "model.load_weights(\"gen_text_weights.h5\")\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, char2idx, idx2char, start_string, generate_char_num=1000, temperature=1.0):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    # Low temperatures results in more predictable text, higher temperatures results in more surprising text.\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []  # Empty string to store our results\n",
    "    model.reset_states()\n",
    "    for i in range(generate_char_num):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)    # remove the batch dimension\n",
    "        predictions /= temperature\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        # We pass the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], axis=0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return start_string + ''.join(text_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of the text, the program will complete it. Your input is: hi there\n",
      "hi there,\n",
      "She shall stand better one by whether means,\n",
      "To three and malies\n",
      "White have dessertied the end mode are sir.\n",
      "\n",
      "SICINIUS:\n",
      "I prepare so, I am beneft to stay.\n",
      "\n",
      "POLIXENES:\n",
      "Pray you, be almost him.\n",
      "Ha!\n",
      "\n",
      "ELBOW:\n",
      "Marry, a nease ded; well! knock, somewhat art\n",
      "All in this people's good, if they no impediment\n",
      "To bear wherein the way by me.\n",
      "\n",
      "MERCUTIO:\n",
      "Will.\n",
      "\n",
      "Third Citizen:\n",
      "Hence but espite of ear shows.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Virtue! or lengs and child is not:\n",
      "Moth's bloody shame! is not perform'd this,\n",
      "I blame thee to my ear,\n",
      "Yet at Lewis sto shame them on.\n",
      "\n",
      "MOPSA:\n",
      "Is lieve, demis presently.\n",
      "\n",
      "MARCIUS:\n",
      "He's a dishonourable hit my will.\n",
      "\n",
      "BAPTISTA:\n",
      "I will, therefore; farewell: had he would, Montague,\n",
      "And I can title my forward is is not\n",
      "send: I cannot get it by the wentleman:\n",
      "Lives me his his ancient shall with\n",
      "The noble suilt did publie like a badded else\n",
      "Spake like a Richard in a subject, or my\n",
      "Greeceiful despair to excent them air,\n",
      "If no law discorder'd with the town of stone:\n",
      "The cift I do beseech you'll needs are so.\n",
      "\n",
      "WARWICK:\n",
      "And had not Mast will\n",
      "therefore the!\n",
      "\n",
      "FRIAR JOHN:\n",
      "Go, get them scorn!\n",
      "\n",
      "YORK:\n",
      "Is a providered betwixt us, all fly with him.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Ay, morening; game unhappy sits,\n",
      "And nothing tr of\n",
      "woe foundly.\n",
      "\n",
      "LARTIUS:\n",
      "So, dost thou noto:\n",
      "The cheaper not against the king and fear the masure,\n",
      "I saw a short so: may the handless damping be so much:\n",
      "My pears, I will bre virtue.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "No, in, sister, madam:\n",
      "Here, the day brother.\n",
      "\n",
      "LUCIO:\n",
      "This well, sir; but he'll way for this sadde Montague,\n",
      "Speed lambs three discourses, not we deny to\n",
      "This Pilga Sebtosing of their chy\n",
      "and were post the rich of his kindness. Peacees, keeping,\n",
      "And bads a drumbling and their creeks\n",
      "So plot with him.\n",
      "\n",
      "Clown:\n",
      "Even are you leave as.\n",
      "\n",
      "MOPSA:\n",
      "I must convent; I for thou reflin\n",
      "As I should wish the man who tells it a doublet fellow?\n",
      "\n",
      "Messenger:\n",
      "Present them to his blood their gods! Shall I be mew it;\n",
      "Nor carvenged I see him?\n",
      "\n",
      "HORTENSIO:\n",
      "I am gone as Tranio, where is my cousin?\n",
      "\n",
      "JU\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Write the beginning of the text, the program will complete it. Your input is: \")\n",
    "generated_text = generate_text(model, char2idx, idx2char, start_string=user_input, generate_char_num=2000)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Colab 5 - Construir una Red Neuronal Recurrente en TensorFlow 2.0.ipynb",
   "provenance": [
    {
     "file_id": "12OM3ntGfd38dUqLJ-Nk82RwKTz3POWGa",
     "timestamp": 1564656346258
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
